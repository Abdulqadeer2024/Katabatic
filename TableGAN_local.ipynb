{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, initializers\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class TableGAN:\n",
    "    def __init__(self, input_dim, generator_dims, discriminator_dims):\n",
    "        self.input_dim = input_dim\n",
    "        self.generator_dims = generator_dims\n",
    "        self.discriminator_dims = discriminator_dims\n",
    "        self.generator = self._make_generator_model()\n",
    "        self.discriminator = self._make_discriminator_model()\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5, beta_2=0.9)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(1e-5, beta_1=0.5, beta_2=0.9)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    def _make_generator_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(layers.Input(shape=(self.input_dim,)))\n",
    "        for dim in self.generator_dims:\n",
    "            model.add(layers.Dense(dim, use_bias=False, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.ReLU())\n",
    "        model.add(layers.Dense(self.input_dim, activation='tanh', kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "        return model\n",
    "\n",
    "    def _make_discriminator_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        for dim in self.discriminator_dims:\n",
    "            if len(model.layers) == 0:\n",
    "                model.add(layers.Dense(dim, input_shape=(self.input_dim,), kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "            else:\n",
    "                model.add(layers.Dense(dim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "            model.add(layers.LeakyReLU(0.2))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "        model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "        return model\n",
    "\n",
    "    def _warmup_run(self, x, y, epochs=1):\n",
    "        lr_model = LogisticRegression(max_iter=1000)\n",
    "        lr_model.fit(x, y)\n",
    "        return lr_model.coef_, lr_model.intercept_\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, real_data):\n",
    "        noise = tf.random.normal([tf.shape(real_data)[0], self.input_dim])\n",
    "        \n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_data = self.generator(noise, training=True)\n",
    "            \n",
    "            real_output = self.discriminator(real_data, training=True)\n",
    "            fake_output = self.discriminator(generated_data, training=True)\n",
    "            \n",
    "            gen_loss = -tf.reduce_mean(fake_output)\n",
    "            disc_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "            \n",
    "            # Add gradient penalty\n",
    "            epsilon = tf.random.uniform([tf.shape(real_data)[0], 1], 0.0, 1.0)\n",
    "            interpolated = epsilon * real_data + (1 - epsilon) * generated_data\n",
    "            with tf.GradientTape() as gp_tape:\n",
    "                gp_tape.watch(interpolated)\n",
    "                interpolated_output = self.discriminator(interpolated, training=True)\n",
    "            grads = gp_tape.gradient(interpolated_output, [interpolated])[0]\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1]))\n",
    "            gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "            disc_loss += 10 * gp\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "\n",
    "        return gen_loss, disc_loss\n",
    "\n",
    "    def fit(self, x, y, epochs=100, batch_size=32, warmup_epochs=1, verbose=0):\n",
    "        # Handle NaN values in x\n",
    "        imputer_x = SimpleImputer(strategy='mean')\n",
    "        x_imputed = imputer_x.fit_transform(x)\n",
    "        \n",
    "        # Handle NaN values in y\n",
    "        imputer_y = SimpleImputer(strategy='most_frequent')\n",
    "        y_imputed = imputer_y.fit_transform(y.reshape(-1, 1)).ravel()\n",
    "\n",
    "        x_scaled = self.scaler.fit_transform(x_imputed).astype(np.float32)\n",
    "        y_encoded = self.onehot_encoder.fit_transform(y_imputed.reshape(-1, 1)).astype(np.float32)\n",
    "        \n",
    "        # Warm-up phase\n",
    "        if verbose:\n",
    "            print(\"Warm-up phase:\")\n",
    "        warmup_weights, warmup_bias = self._warmup_run(x_scaled, y_imputed, warmup_epochs)\n",
    "        \n",
    "        # Initialize part of the generator's final layer with warm-up weights\n",
    "        gen_final_layer = self.generator.layers[-1]\n",
    "        current_weights, current_bias = gen_final_layer.get_weights()\n",
    "        \n",
    "        # Adjust the warm-up weights to match the shape of the generator's final layer\n",
    "        adjusted_warmup_weights = np.zeros_like(current_weights)\n",
    "        adjusted_warmup_weights[:warmup_weights.shape[1], :warmup_weights.shape[0]] = warmup_weights.T\n",
    "        \n",
    "        # Combine the adjusted warm-up weights with the existing weights\n",
    "        new_weights = 0.5 * current_weights + 0.5 * adjusted_warmup_weights\n",
    "        \n",
    "        new_bias = current_bias.copy()\n",
    "        new_bias[:warmup_bias.shape[0]] = warmup_bias\n",
    "        gen_final_layer.set_weights([new_weights, new_bias])\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((x_scaled, y_encoded)).shuffle(buffer_size=1000).batch(batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_data, _ in dataset:\n",
    "                gen_loss, disc_loss = self.train_step(batch_data)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch+1}, Generator Loss: {gen_loss.numpy()}, Discriminator Loss: {disc_loss.numpy()}\")\n",
    "\n",
    "    def generate(self, n_samples):\n",
    "        noise = tf.random.normal([n_samples, self.input_dim])\n",
    "        generated_data = self.generator(noise, training=False)\n",
    "        generated_data_scaled = self.scaler.inverse_transform(generated_data.numpy())\n",
    "        return np.clip(generated_data_scaled, 0, 1)  # Clip values between 0 and 1\n",
    "\n",
    "    def evaluate(self, x, y, model='lr'):\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        models = {\n",
    "            'lr': LogisticRegression,\n",
    "            'mlp': MLPClassifier,\n",
    "            'rf': RandomForestClassifier\n",
    "        }\n",
    "        \n",
    "        if model not in models:\n",
    "            raise ValueError(\"Invalid model. Choose from 'lr', 'mlp', or 'rf'.\")\n",
    "        \n",
    "        eval_model = models[model]()\n",
    "        \n",
    "        synthetic_data = self.generate(len(x))\n",
    "        synthetic_x, synthetic_y = synthetic_data[:, :-1], synthetic_data[:, -1]\n",
    "        \n",
    "        eval_model.fit(synthetic_x, synthetic_y)\n",
    "        y_pred = eval_model.predict(x)\n",
    "        \n",
    "        return accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the datasets\n",
    "features = pd.read_csv('train_Adult_cleaned.csv')\n",
    "labels = pd.read_csv('train_Adult_labels.csv')\n",
    "\n",
    "# Assuming labels are in a single column and the rows correspond to the features\n",
    "data = pd.concat([features, labels], axis=1)\n",
    "\n",
    "# Preprocess the dataset\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(data_scaled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 15)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tahat\\anaconda3\\envs\\chatbot\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up phase:\n",
      "Epoch 1, Generator Loss: -0.5000630021095276, Discriminator Loss: 9.992368698120117\n",
      "Epoch 1, Generator Loss: -0.5000796318054199, Discriminator Loss: 9.992104530334473\n",
      "Epoch 1, Generator Loss: -0.500055730342865, Discriminator Loss: 9.99273681640625\n",
      "Epoch 1, Generator Loss: -0.5000489950180054, Discriminator Loss: 9.99250602722168\n",
      "Epoch 1, Generator Loss: -0.5000627040863037, Discriminator Loss: 9.992541313171387\n",
      "Epoch 1, Generator Loss: -0.5000782012939453, Discriminator Loss: 9.992025375366211\n",
      "Epoch 1, Generator Loss: -0.5000792741775513, Discriminator Loss: 9.992410659790039\n",
      "Epoch 1, Generator Loss: -0.5000555515289307, Discriminator Loss: 9.992036819458008\n",
      "Epoch 1, Generator Loss: -0.5000478029251099, Discriminator Loss: 9.992213249206543\n",
      "Epoch 1, Generator Loss: -0.5000741481781006, Discriminator Loss: 9.992563247680664\n",
      "Epoch 1, Generator Loss: -0.5000461339950562, Discriminator Loss: 9.991708755493164\n",
      "Epoch 1, Generator Loss: -0.500052273273468, Discriminator Loss: 9.992175102233887\n",
      "Epoch 1, Generator Loss: -0.5000622272491455, Discriminator Loss: 9.992111206054688\n",
      "Epoch 1, Generator Loss: -0.5000537633895874, Discriminator Loss: 9.99199104309082\n",
      "Epoch 1, Generator Loss: -0.500052809715271, Discriminator Loss: 9.991931915283203\n",
      "Epoch 1, Generator Loss: -0.5000439882278442, Discriminator Loss: 9.991340637207031\n",
      "Epoch 1, Generator Loss: -0.5000811815261841, Discriminator Loss: 9.991690635681152\n",
      "Epoch 1, Generator Loss: -0.5000834465026855, Discriminator Loss: 9.991642951965332\n",
      "Epoch 1, Generator Loss: -0.5000779032707214, Discriminator Loss: 9.991386413574219\n",
      "Epoch 1, Generator Loss: -0.5000810623168945, Discriminator Loss: 9.990943908691406\n",
      "Epoch 1, Generator Loss: -0.5000640153884888, Discriminator Loss: 9.990883827209473\n",
      "Epoch 1, Generator Loss: -0.500084400177002, Discriminator Loss: 9.991432189941406\n",
      "Epoch 1, Generator Loss: -0.5000636577606201, Discriminator Loss: 9.990894317626953\n",
      "Epoch 1, Generator Loss: -0.5000776052474976, Discriminator Loss: 9.990471839904785\n",
      "Epoch 1, Generator Loss: -0.500075101852417, Discriminator Loss: 9.990650177001953\n",
      "Epoch 1, Generator Loss: -0.5000930428504944, Discriminator Loss: 9.990595817565918\n",
      "Epoch 1, Generator Loss: -0.5000513792037964, Discriminator Loss: 9.990457534790039\n",
      "Epoch 1, Generator Loss: -0.5000776052474976, Discriminator Loss: 9.99047565460205\n",
      "Epoch 1, Generator Loss: -0.5000632405281067, Discriminator Loss: 9.990289688110352\n",
      "Epoch 1, Generator Loss: -0.5000711679458618, Discriminator Loss: 9.990381240844727\n",
      "Epoch 1, Generator Loss: -0.5000747442245483, Discriminator Loss: 9.989930152893066\n",
      "Epoch 1, Generator Loss: -0.5000790357589722, Discriminator Loss: 9.99043083190918\n",
      "Epoch 1, Generator Loss: -0.5000950694084167, Discriminator Loss: 9.990436553955078\n",
      "Epoch 1, Generator Loss: -0.5001173615455627, Discriminator Loss: 9.990139961242676\n",
      "Epoch 1, Generator Loss: -0.5001287460327148, Discriminator Loss: 9.989262580871582\n",
      "Epoch 1, Generator Loss: -0.500103235244751, Discriminator Loss: 9.989980697631836\n",
      "Epoch 1, Generator Loss: -0.5000872611999512, Discriminator Loss: 9.99008560180664\n",
      "Epoch 1, Generator Loss: -0.5001184940338135, Discriminator Loss: 9.98884105682373\n",
      "Epoch 1, Generator Loss: -0.5000905394554138, Discriminator Loss: 9.989282608032227\n",
      "Epoch 1, Generator Loss: -0.5000916123390198, Discriminator Loss: 9.988879203796387\n",
      "Epoch 1, Generator Loss: -0.5001029372215271, Discriminator Loss: 9.989871978759766\n",
      "Epoch 1, Generator Loss: -0.5001094341278076, Discriminator Loss: 9.98896598815918\n",
      "Epoch 1, Generator Loss: -0.5001437067985535, Discriminator Loss: 9.989079475402832\n",
      "Epoch 1, Generator Loss: -0.5000923871994019, Discriminator Loss: 9.988053321838379\n",
      "Epoch 1, Generator Loss: -0.5001415014266968, Discriminator Loss: 9.98824405670166\n",
      "Epoch 1, Generator Loss: -0.5001373291015625, Discriminator Loss: 9.988787651062012\n",
      "Epoch 1, Generator Loss: -0.5001252293586731, Discriminator Loss: 9.987536430358887\n",
      "Epoch 1, Generator Loss: -0.5001447200775146, Discriminator Loss: 9.9882230758667\n",
      "Epoch 1, Generator Loss: -0.5001124143600464, Discriminator Loss: 9.987557411193848\n",
      "Epoch 1, Generator Loss: -0.5001460909843445, Discriminator Loss: 9.988160133361816\n",
      "Epoch 2, Generator Loss: -0.5001329183578491, Discriminator Loss: 9.988302230834961\n",
      "Epoch 2, Generator Loss: -0.50016188621521, Discriminator Loss: 9.987730979919434\n",
      "Epoch 2, Generator Loss: -0.5001603364944458, Discriminator Loss: 9.986671447753906\n",
      "Epoch 2, Generator Loss: -0.5001416802406311, Discriminator Loss: 9.985857963562012\n",
      "Epoch 2, Generator Loss: -0.5001376867294312, Discriminator Loss: 9.986780166625977\n",
      "Epoch 2, Generator Loss: -0.5001667737960815, Discriminator Loss: 9.986303329467773\n",
      "Epoch 2, Generator Loss: -0.50017911195755, Discriminator Loss: 9.987348556518555\n",
      "Epoch 2, Generator Loss: -0.5001776814460754, Discriminator Loss: 9.98764705657959\n",
      "Epoch 2, Generator Loss: -0.5001835227012634, Discriminator Loss: 9.986482620239258\n",
      "Epoch 2, Generator Loss: -0.5002107620239258, Discriminator Loss: 9.986249923706055\n",
      "Epoch 2, Generator Loss: -0.5002215504646301, Discriminator Loss: 9.98656177520752\n",
      "Epoch 2, Generator Loss: -0.5002254247665405, Discriminator Loss: 9.98584270477295\n",
      "Epoch 2, Generator Loss: -0.5001855492591858, Discriminator Loss: 9.986172676086426\n",
      "Epoch 2, Generator Loss: -0.5001986026763916, Discriminator Loss: 9.984951972961426\n",
      "Epoch 2, Generator Loss: -0.5002491474151611, Discriminator Loss: 9.986010551452637\n",
      "Epoch 2, Generator Loss: -0.5002337098121643, Discriminator Loss: 9.985143661499023\n",
      "Epoch 2, Generator Loss: -0.5002318024635315, Discriminator Loss: 9.984626770019531\n",
      "Epoch 2, Generator Loss: -0.5002802014350891, Discriminator Loss: 9.985304832458496\n",
      "Epoch 2, Generator Loss: -0.5002480745315552, Discriminator Loss: 9.98491382598877\n",
      "Epoch 2, Generator Loss: -0.5002646446228027, Discriminator Loss: 9.983972549438477\n",
      "Epoch 2, Generator Loss: -0.500335156917572, Discriminator Loss: 9.985052108764648\n",
      "Epoch 2, Generator Loss: -0.5003035664558411, Discriminator Loss: 9.983294486999512\n",
      "Epoch 2, Generator Loss: -0.5003209710121155, Discriminator Loss: 9.984193801879883\n",
      "Epoch 2, Generator Loss: -0.5003229975700378, Discriminator Loss: 9.983271598815918\n",
      "Epoch 2, Generator Loss: -0.500278890132904, Discriminator Loss: 9.98385238647461\n",
      "Epoch 2, Generator Loss: -0.5003101229667664, Discriminator Loss: 9.982640266418457\n",
      "Epoch 2, Generator Loss: -0.5002849698066711, Discriminator Loss: 9.983027458190918\n",
      "Epoch 2, Generator Loss: -0.5003523826599121, Discriminator Loss: 9.983363151550293\n",
      "Epoch 2, Generator Loss: -0.500339925289154, Discriminator Loss: 9.983277320861816\n",
      "Epoch 2, Generator Loss: -0.5003359317779541, Discriminator Loss: 9.98275089263916\n",
      "Epoch 2, Generator Loss: -0.5003483295440674, Discriminator Loss: 9.981785774230957\n",
      "Epoch 2, Generator Loss: -0.5003398656845093, Discriminator Loss: 9.980924606323242\n",
      "Epoch 2, Generator Loss: -0.5003418922424316, Discriminator Loss: 9.981639862060547\n",
      "Epoch 2, Generator Loss: -0.5004065036773682, Discriminator Loss: 9.981975555419922\n",
      "Epoch 2, Generator Loss: -0.5003441572189331, Discriminator Loss: 9.981223106384277\n",
      "Epoch 2, Generator Loss: -0.500382661819458, Discriminator Loss: 9.980560302734375\n",
      "Epoch 2, Generator Loss: -0.5003924369812012, Discriminator Loss: 9.980975151062012\n",
      "Epoch 2, Generator Loss: -0.5004191994667053, Discriminator Loss: 9.980920791625977\n",
      "Epoch 2, Generator Loss: -0.500469446182251, Discriminator Loss: 9.980341911315918\n",
      "Epoch 2, Generator Loss: -0.5004339218139648, Discriminator Loss: 9.981081008911133\n",
      "Epoch 2, Generator Loss: -0.5004415512084961, Discriminator Loss: 9.980539321899414\n",
      "Epoch 2, Generator Loss: -0.5004265308380127, Discriminator Loss: 9.979211807250977\n",
      "Epoch 2, Generator Loss: -0.5004791617393494, Discriminator Loss: 9.979636192321777\n",
      "Epoch 2, Generator Loss: -0.5005043745040894, Discriminator Loss: 9.98031234741211\n",
      "Epoch 2, Generator Loss: -0.5004620552062988, Discriminator Loss: 9.978959083557129\n",
      "Epoch 2, Generator Loss: -0.5004876255989075, Discriminator Loss: 9.97830867767334\n",
      "Epoch 2, Generator Loss: -0.5005329847335815, Discriminator Loss: 9.979304313659668\n",
      "Epoch 2, Generator Loss: -0.500598669052124, Discriminator Loss: 9.980169296264648\n",
      "Epoch 2, Generator Loss: -0.5005608797073364, Discriminator Loss: 9.979199409484863\n",
      "Epoch 2, Generator Loss: -0.5005977153778076, Discriminator Loss: 9.97983169555664\n",
      "Epoch 3, Generator Loss: -0.5006099343299866, Discriminator Loss: 9.978363037109375\n",
      "Epoch 3, Generator Loss: -0.5005680322647095, Discriminator Loss: 9.976840019226074\n",
      "Epoch 3, Generator Loss: -0.500617504119873, Discriminator Loss: 9.977341651916504\n",
      "Epoch 3, Generator Loss: -0.5005680322647095, Discriminator Loss: 9.978561401367188\n",
      "Epoch 3, Generator Loss: -0.5005964636802673, Discriminator Loss: 9.976067543029785\n",
      "Epoch 3, Generator Loss: -0.5006734132766724, Discriminator Loss: 9.975890159606934\n",
      "Epoch 3, Generator Loss: -0.5006536245346069, Discriminator Loss: 9.977259635925293\n",
      "Epoch 3, Generator Loss: -0.5006812810897827, Discriminator Loss: 9.976672172546387\n",
      "Epoch 3, Generator Loss: -0.5007233619689941, Discriminator Loss: 9.976556777954102\n",
      "Epoch 3, Generator Loss: -0.5006943941116333, Discriminator Loss: 9.975830078125\n",
      "Epoch 3, Generator Loss: -0.5007795095443726, Discriminator Loss: 9.976211547851562\n",
      "Epoch 3, Generator Loss: -0.5008188486099243, Discriminator Loss: 9.977779388427734\n",
      "Epoch 3, Generator Loss: -0.5007717609405518, Discriminator Loss: 9.974472999572754\n",
      "Epoch 3, Generator Loss: -0.5007622241973877, Discriminator Loss: 9.974520683288574\n",
      "Epoch 3, Generator Loss: -0.5009241104125977, Discriminator Loss: 9.973993301391602\n",
      "Epoch 3, Generator Loss: -0.5008044242858887, Discriminator Loss: 9.974687576293945\n",
      "Epoch 3, Generator Loss: -0.5008029937744141, Discriminator Loss: 9.973319053649902\n",
      "Epoch 3, Generator Loss: -0.5007301568984985, Discriminator Loss: 9.974876403808594\n",
      "Epoch 3, Generator Loss: -0.5008831024169922, Discriminator Loss: 9.975214004516602\n",
      "Epoch 3, Generator Loss: -0.5008150339126587, Discriminator Loss: 9.97491455078125\n",
      "Epoch 3, Generator Loss: -0.5008947849273682, Discriminator Loss: 9.974449157714844\n",
      "Epoch 3, Generator Loss: -0.5009227991104126, Discriminator Loss: 9.974172592163086\n",
      "Epoch 3, Generator Loss: -0.5009382963180542, Discriminator Loss: 9.974656105041504\n",
      "Epoch 3, Generator Loss: -0.5009646415710449, Discriminator Loss: 9.970636367797852\n",
      "Epoch 3, Generator Loss: -0.5009536743164062, Discriminator Loss: 9.972768783569336\n",
      "Epoch 3, Generator Loss: -0.5010164976119995, Discriminator Loss: 9.970727920532227\n",
      "Epoch 3, Generator Loss: -0.5010923147201538, Discriminator Loss: 9.971627235412598\n",
      "Epoch 3, Generator Loss: -0.5009140968322754, Discriminator Loss: 9.972188949584961\n",
      "Epoch 3, Generator Loss: -0.5010573863983154, Discriminator Loss: 9.972282409667969\n",
      "Epoch 3, Generator Loss: -0.5011196732521057, Discriminator Loss: 9.970288276672363\n",
      "Epoch 3, Generator Loss: -0.5011307597160339, Discriminator Loss: 9.970895767211914\n",
      "Epoch 3, Generator Loss: -0.5010874271392822, Discriminator Loss: 9.96910572052002\n",
      "Epoch 3, Generator Loss: -0.5011130571365356, Discriminator Loss: 9.96827507019043\n",
      "Epoch 3, Generator Loss: -0.50117427110672, Discriminator Loss: 9.96995735168457\n",
      "Epoch 3, Generator Loss: -0.5011428594589233, Discriminator Loss: 9.969897270202637\n",
      "Epoch 3, Generator Loss: -0.5011401176452637, Discriminator Loss: 9.969605445861816\n",
      "Epoch 3, Generator Loss: -0.5013415813446045, Discriminator Loss: 9.968408584594727\n",
      "Epoch 3, Generator Loss: -0.5013439655303955, Discriminator Loss: 9.967967987060547\n",
      "Epoch 3, Generator Loss: -0.5013227462768555, Discriminator Loss: 9.967897415161133\n",
      "Epoch 3, Generator Loss: -0.5012389421463013, Discriminator Loss: 9.966650009155273\n",
      "Epoch 3, Generator Loss: -0.5014634728431702, Discriminator Loss: 9.967957496643066\n",
      "Epoch 3, Generator Loss: -0.501359760761261, Discriminator Loss: 9.967584609985352\n",
      "Epoch 3, Generator Loss: -0.5013803243637085, Discriminator Loss: 9.968066215515137\n",
      "Epoch 3, Generator Loss: -0.5014183521270752, Discriminator Loss: 9.967148780822754\n",
      "Epoch 3, Generator Loss: -0.5014591813087463, Discriminator Loss: 9.966001510620117\n",
      "Epoch 3, Generator Loss: -0.5015556216239929, Discriminator Loss: 9.963879585266113\n",
      "Epoch 3, Generator Loss: -0.5015008449554443, Discriminator Loss: 9.964141845703125\n",
      "Epoch 3, Generator Loss: -0.5015184879302979, Discriminator Loss: 9.963213920593262\n",
      "Epoch 3, Generator Loss: -0.5014718174934387, Discriminator Loss: 9.96367359161377\n",
      "Epoch 3, Generator Loss: -0.5015716552734375, Discriminator Loss: 9.963218688964844\n",
      "Epoch 4, Generator Loss: -0.501732349395752, Discriminator Loss: 9.964062690734863\n",
      "Epoch 4, Generator Loss: -0.5016613006591797, Discriminator Loss: 9.963294982910156\n",
      "Epoch 4, Generator Loss: -0.5016930103302002, Discriminator Loss: 9.963889122009277\n",
      "Epoch 4, Generator Loss: -0.501672089099884, Discriminator Loss: 9.962589263916016\n",
      "Epoch 4, Generator Loss: -0.5017824769020081, Discriminator Loss: 9.963729858398438\n",
      "Epoch 4, Generator Loss: -0.5016616582870483, Discriminator Loss: 9.960989952087402\n",
      "Epoch 4, Generator Loss: -0.5017935037612915, Discriminator Loss: 9.962550163269043\n",
      "Epoch 4, Generator Loss: -0.5018352270126343, Discriminator Loss: 9.963499069213867\n",
      "Epoch 4, Generator Loss: -0.5019387006759644, Discriminator Loss: 9.961040496826172\n",
      "Epoch 4, Generator Loss: -0.5019898414611816, Discriminator Loss: 9.962573051452637\n",
      "Epoch 4, Generator Loss: -0.5018922090530396, Discriminator Loss: 9.960545539855957\n",
      "Epoch 4, Generator Loss: -0.5019699335098267, Discriminator Loss: 9.963284492492676\n",
      "Epoch 4, Generator Loss: -0.5021021962165833, Discriminator Loss: 9.96345043182373\n",
      "Epoch 4, Generator Loss: -0.5019689202308655, Discriminator Loss: 9.959128379821777\n",
      "Epoch 4, Generator Loss: -0.5020110607147217, Discriminator Loss: 9.958451271057129\n",
      "Epoch 4, Generator Loss: -0.5020831823348999, Discriminator Loss: 9.961816787719727\n",
      "Epoch 4, Generator Loss: -0.5019636154174805, Discriminator Loss: 9.961908340454102\n",
      "Epoch 4, Generator Loss: -0.5020841956138611, Discriminator Loss: 9.958649635314941\n",
      "Epoch 4, Generator Loss: -0.5021985173225403, Discriminator Loss: 9.958005905151367\n",
      "Epoch 4, Generator Loss: -0.5020950436592102, Discriminator Loss: 9.959064483642578\n",
      "Epoch 4, Generator Loss: -0.5021754503250122, Discriminator Loss: 9.957883834838867\n",
      "Epoch 4, Generator Loss: -0.5023972392082214, Discriminator Loss: 9.956873893737793\n",
      "Epoch 4, Generator Loss: -0.502262532711029, Discriminator Loss: 9.955365180969238\n",
      "Epoch 4, Generator Loss: -0.5022710561752319, Discriminator Loss: 9.95510482788086\n",
      "Epoch 4, Generator Loss: -0.5024283528327942, Discriminator Loss: 9.953713417053223\n",
      "Epoch 4, Generator Loss: -0.5025020837783813, Discriminator Loss: 9.956550598144531\n",
      "Epoch 4, Generator Loss: -0.5023015141487122, Discriminator Loss: 9.957212448120117\n",
      "Epoch 4, Generator Loss: -0.502364456653595, Discriminator Loss: 9.954765319824219\n",
      "Epoch 4, Generator Loss: -0.5024445056915283, Discriminator Loss: 9.951632499694824\n",
      "Epoch 4, Generator Loss: -0.5024139881134033, Discriminator Loss: 9.954669952392578\n",
      "Epoch 4, Generator Loss: -0.5026853084564209, Discriminator Loss: 9.952364921569824\n",
      "Epoch 4, Generator Loss: -0.5025179386138916, Discriminator Loss: 9.95425033569336\n",
      "Epoch 4, Generator Loss: -0.5025724768638611, Discriminator Loss: 9.954734802246094\n",
      "Epoch 4, Generator Loss: -0.5026907920837402, Discriminator Loss: 9.952829360961914\n",
      "Epoch 4, Generator Loss: -0.5027430653572083, Discriminator Loss: 9.949515342712402\n",
      "Epoch 4, Generator Loss: -0.5027540922164917, Discriminator Loss: 9.951081275939941\n",
      "Epoch 4, Generator Loss: -0.5029270648956299, Discriminator Loss: 9.951363563537598\n",
      "Epoch 4, Generator Loss: -0.5028098821640015, Discriminator Loss: 9.95191478729248\n",
      "Epoch 4, Generator Loss: -0.5030114650726318, Discriminator Loss: 9.952518463134766\n",
      "Epoch 4, Generator Loss: -0.5028699040412903, Discriminator Loss: 9.949396133422852\n",
      "Epoch 4, Generator Loss: -0.5030152797698975, Discriminator Loss: 9.946671485900879\n",
      "Epoch 4, Generator Loss: -0.5030288696289062, Discriminator Loss: 9.947183609008789\n",
      "Epoch 4, Generator Loss: -0.5031070709228516, Discriminator Loss: 9.950180053710938\n",
      "Epoch 4, Generator Loss: -0.5030749440193176, Discriminator Loss: 9.949657440185547\n",
      "Epoch 4, Generator Loss: -0.5030695199966431, Discriminator Loss: 9.949324607849121\n",
      "Epoch 4, Generator Loss: -0.5032567977905273, Discriminator Loss: 9.94804573059082\n",
      "Epoch 4, Generator Loss: -0.5031458139419556, Discriminator Loss: 9.946329116821289\n",
      "Epoch 4, Generator Loss: -0.5036532878875732, Discriminator Loss: 9.949216842651367\n",
      "Epoch 4, Generator Loss: -0.5033204555511475, Discriminator Loss: 9.946023941040039\n",
      "Epoch 4, Generator Loss: -0.5033929944038391, Discriminator Loss: 9.946935653686523\n",
      "Epoch 5, Generator Loss: -0.5035001039505005, Discriminator Loss: 9.943971633911133\n",
      "Epoch 5, Generator Loss: -0.5034081339836121, Discriminator Loss: 9.946043968200684\n",
      "Epoch 5, Generator Loss: -0.5037223100662231, Discriminator Loss: 9.948108673095703\n",
      "Epoch 5, Generator Loss: -0.5034462213516235, Discriminator Loss: 9.944104194641113\n",
      "Epoch 5, Generator Loss: -0.5034571886062622, Discriminator Loss: 9.943161964416504\n",
      "Epoch 5, Generator Loss: -0.503684401512146, Discriminator Loss: 9.942960739135742\n",
      "Epoch 5, Generator Loss: -0.5037931203842163, Discriminator Loss: 9.939175605773926\n",
      "Epoch 5, Generator Loss: -0.5038615465164185, Discriminator Loss: 9.941604614257812\n",
      "Epoch 5, Generator Loss: -0.5038248896598816, Discriminator Loss: 9.945089340209961\n",
      "Epoch 5, Generator Loss: -0.5040746927261353, Discriminator Loss: 9.938652038574219\n",
      "Epoch 5, Generator Loss: -0.5039608478546143, Discriminator Loss: 9.936788558959961\n",
      "Epoch 5, Generator Loss: -0.5042209625244141, Discriminator Loss: 9.939431190490723\n",
      "Epoch 5, Generator Loss: -0.5039264559745789, Discriminator Loss: 9.938301086425781\n",
      "Epoch 5, Generator Loss: -0.5040202140808105, Discriminator Loss: 9.940238952636719\n",
      "Epoch 5, Generator Loss: -0.503928542137146, Discriminator Loss: 9.938339233398438\n",
      "Epoch 5, Generator Loss: -0.5040926933288574, Discriminator Loss: 9.938834190368652\n",
      "Epoch 5, Generator Loss: -0.5040837526321411, Discriminator Loss: 9.937826156616211\n",
      "Epoch 5, Generator Loss: -0.504002034664154, Discriminator Loss: 9.935422897338867\n",
      "Epoch 5, Generator Loss: -0.5046495199203491, Discriminator Loss: 9.932757377624512\n",
      "Epoch 5, Generator Loss: -0.5044126510620117, Discriminator Loss: 9.935837745666504\n",
      "Epoch 5, Generator Loss: -0.5045396685600281, Discriminator Loss: 9.932640075683594\n",
      "Epoch 5, Generator Loss: -0.5043879151344299, Discriminator Loss: 9.936844825744629\n",
      "Epoch 5, Generator Loss: -0.5046873688697815, Discriminator Loss: 9.933222770690918\n",
      "Epoch 5, Generator Loss: -0.504588782787323, Discriminator Loss: 9.933565139770508\n",
      "Epoch 5, Generator Loss: -0.504596471786499, Discriminator Loss: 9.928668022155762\n",
      "Epoch 5, Generator Loss: -0.5046481490135193, Discriminator Loss: 9.931642532348633\n",
      "Epoch 5, Generator Loss: -0.5050296187400818, Discriminator Loss: 9.93197250366211\n",
      "Epoch 5, Generator Loss: -0.5048688650131226, Discriminator Loss: 9.929211616516113\n",
      "Epoch 5, Generator Loss: -0.5049590468406677, Discriminator Loss: 9.934462547302246\n",
      "Epoch 5, Generator Loss: -0.5051226019859314, Discriminator Loss: 9.931528091430664\n",
      "Epoch 5, Generator Loss: -0.5051117539405823, Discriminator Loss: 9.93033218383789\n",
      "Epoch 5, Generator Loss: -0.5051639676094055, Discriminator Loss: 9.929708480834961\n",
      "Epoch 5, Generator Loss: -0.5051048994064331, Discriminator Loss: 9.92881965637207\n",
      "Epoch 5, Generator Loss: -0.5050793886184692, Discriminator Loss: 9.929173469543457\n",
      "Epoch 5, Generator Loss: -0.5052998065948486, Discriminator Loss: 9.927813529968262\n",
      "Epoch 5, Generator Loss: -0.5054599642753601, Discriminator Loss: 9.928985595703125\n",
      "Epoch 5, Generator Loss: -0.5054375529289246, Discriminator Loss: 9.922453880310059\n",
      "Epoch 5, Generator Loss: -0.5054037570953369, Discriminator Loss: 9.923151969909668\n",
      "Epoch 5, Generator Loss: -0.5055489540100098, Discriminator Loss: 9.926966667175293\n",
      "Epoch 5, Generator Loss: -0.505428671836853, Discriminator Loss: 9.923670768737793\n",
      "Epoch 5, Generator Loss: -0.5056455135345459, Discriminator Loss: 9.926647186279297\n",
      "Epoch 5, Generator Loss: -0.5057845115661621, Discriminator Loss: 9.923224449157715\n",
      "Epoch 5, Generator Loss: -0.5055136680603027, Discriminator Loss: 9.920818328857422\n",
      "Epoch 5, Generator Loss: -0.5055159330368042, Discriminator Loss: 9.921048164367676\n",
      "Epoch 5, Generator Loss: -0.5059319734573364, Discriminator Loss: 9.918746948242188\n",
      "Epoch 5, Generator Loss: -0.5058760643005371, Discriminator Loss: 9.922032356262207\n",
      "Epoch 5, Generator Loss: -0.506272554397583, Discriminator Loss: 9.914168357849121\n",
      "Epoch 5, Generator Loss: -0.5063159465789795, Discriminator Loss: 9.923322677612305\n",
      "Epoch 5, Generator Loss: -0.5062174797058105, Discriminator Loss: 9.91958999633789\n",
      "Epoch 5, Generator Loss: -0.506200909614563, Discriminator Loss: 9.91711711883545\n",
      "Epoch 6, Generator Loss: -0.5064296722412109, Discriminator Loss: 9.918195724487305\n",
      "Epoch 6, Generator Loss: -0.5062062740325928, Discriminator Loss: 9.918743133544922\n",
      "Epoch 6, Generator Loss: -0.5064694881439209, Discriminator Loss: 9.917393684387207\n",
      "Epoch 6, Generator Loss: -0.5061664581298828, Discriminator Loss: 9.913484573364258\n",
      "Epoch 6, Generator Loss: -0.5063718557357788, Discriminator Loss: 9.914634704589844\n",
      "Epoch 6, Generator Loss: -0.5067281126976013, Discriminator Loss: 9.917245864868164\n",
      "Epoch 6, Generator Loss: -0.5064420700073242, Discriminator Loss: 9.915454864501953\n",
      "Epoch 6, Generator Loss: -0.5071669816970825, Discriminator Loss: 9.91895580291748\n",
      "Epoch 6, Generator Loss: -0.5070742964744568, Discriminator Loss: 9.912196159362793\n",
      "Epoch 6, Generator Loss: -0.5071559548377991, Discriminator Loss: 9.91428279876709\n",
      "Epoch 6, Generator Loss: -0.5075136423110962, Discriminator Loss: 9.911717414855957\n",
      "Epoch 6, Generator Loss: -0.5073181390762329, Discriminator Loss: 9.911993026733398\n",
      "Epoch 6, Generator Loss: -0.507280707359314, Discriminator Loss: 9.910379409790039\n",
      "Epoch 6, Generator Loss: -0.5072534084320068, Discriminator Loss: 9.910613059997559\n",
      "Epoch 6, Generator Loss: -0.5072031021118164, Discriminator Loss: 9.908716201782227\n",
      "Epoch 6, Generator Loss: -0.5075249671936035, Discriminator Loss: 9.912212371826172\n",
      "Epoch 6, Generator Loss: -0.5075702667236328, Discriminator Loss: 9.910484313964844\n",
      "Epoch 6, Generator Loss: -0.5076700448989868, Discriminator Loss: 9.909876823425293\n",
      "Epoch 6, Generator Loss: -0.507901668548584, Discriminator Loss: 9.9079008102417\n",
      "Epoch 6, Generator Loss: -0.50828617811203, Discriminator Loss: 9.911521911621094\n",
      "Epoch 6, Generator Loss: -0.5074490904808044, Discriminator Loss: 9.902502059936523\n",
      "Epoch 6, Generator Loss: -0.5081192255020142, Discriminator Loss: 9.903666496276855\n",
      "Epoch 6, Generator Loss: -0.5084638595581055, Discriminator Loss: 9.907310485839844\n",
      "Epoch 6, Generator Loss: -0.5082279443740845, Discriminator Loss: 9.901801109313965\n",
      "Epoch 6, Generator Loss: -0.5084313750267029, Discriminator Loss: 9.899863243103027\n",
      "Epoch 6, Generator Loss: -0.5078984498977661, Discriminator Loss: 9.903244018554688\n",
      "Epoch 6, Generator Loss: -0.5083507299423218, Discriminator Loss: 9.903175354003906\n",
      "Epoch 6, Generator Loss: -0.508556604385376, Discriminator Loss: 9.90760612487793\n",
      "Epoch 6, Generator Loss: -0.508665919303894, Discriminator Loss: 9.898336410522461\n",
      "Epoch 6, Generator Loss: -0.5084619522094727, Discriminator Loss: 9.900806427001953\n",
      "Epoch 6, Generator Loss: -0.5085403323173523, Discriminator Loss: 9.897906303405762\n",
      "Epoch 6, Generator Loss: -0.509207546710968, Discriminator Loss: 9.897514343261719\n",
      "Epoch 6, Generator Loss: -0.5085340142250061, Discriminator Loss: 9.892247200012207\n",
      "Epoch 6, Generator Loss: -0.5091872811317444, Discriminator Loss: 9.894379615783691\n",
      "Epoch 6, Generator Loss: -0.5089325904846191, Discriminator Loss: 9.896111488342285\n",
      "Epoch 6, Generator Loss: -0.5087002515792847, Discriminator Loss: 9.89796257019043\n",
      "Epoch 6, Generator Loss: -0.5096811056137085, Discriminator Loss: 9.902148246765137\n",
      "Epoch 6, Generator Loss: -0.5100072622299194, Discriminator Loss: 9.89221477508545\n",
      "Epoch 6, Generator Loss: -0.5097077488899231, Discriminator Loss: 9.888836860656738\n",
      "Epoch 6, Generator Loss: -0.5097213983535767, Discriminator Loss: 9.883901596069336\n",
      "Epoch 6, Generator Loss: -0.5097840428352356, Discriminator Loss: 9.88989543914795\n",
      "Epoch 6, Generator Loss: -0.5095562934875488, Discriminator Loss: 9.889082908630371\n",
      "Epoch 6, Generator Loss: -0.510088324546814, Discriminator Loss: 9.886881828308105\n",
      "Epoch 6, Generator Loss: -0.5098954439163208, Discriminator Loss: 9.885601043701172\n",
      "Epoch 6, Generator Loss: -0.5102634429931641, Discriminator Loss: 9.885354995727539\n",
      "Epoch 6, Generator Loss: -0.5106128454208374, Discriminator Loss: 9.887749671936035\n",
      "Epoch 6, Generator Loss: -0.5101756453514099, Discriminator Loss: 9.884422302246094\n",
      "Epoch 6, Generator Loss: -0.51121985912323, Discriminator Loss: 9.88424301147461\n",
      "Epoch 6, Generator Loss: -0.510857343673706, Discriminator Loss: 9.894614219665527\n",
      "Epoch 6, Generator Loss: -0.5104045867919922, Discriminator Loss: 9.887531280517578\n",
      "Epoch 7, Generator Loss: -0.5106610059738159, Discriminator Loss: 9.88463020324707\n",
      "Epoch 7, Generator Loss: -0.5107389688491821, Discriminator Loss: 9.881044387817383\n",
      "Epoch 7, Generator Loss: -0.5109224319458008, Discriminator Loss: 9.882994651794434\n",
      "Epoch 7, Generator Loss: -0.5115351676940918, Discriminator Loss: 9.890192985534668\n",
      "Epoch 7, Generator Loss: -0.5110909938812256, Discriminator Loss: 9.879261016845703\n",
      "Epoch 7, Generator Loss: -0.510811984539032, Discriminator Loss: 9.881590843200684\n",
      "Epoch 7, Generator Loss: -0.5109679698944092, Discriminator Loss: 9.875917434692383\n",
      "Epoch 7, Generator Loss: -0.5116478204727173, Discriminator Loss: 9.879618644714355\n",
      "Epoch 7, Generator Loss: -0.5112308859825134, Discriminator Loss: 9.87973690032959\n",
      "Epoch 7, Generator Loss: -0.5120847225189209, Discriminator Loss: 9.880780220031738\n",
      "Epoch 7, Generator Loss: -0.5121954679489136, Discriminator Loss: 9.876336097717285\n",
      "Epoch 7, Generator Loss: -0.5121338963508606, Discriminator Loss: 9.884373664855957\n",
      "Epoch 7, Generator Loss: -0.5122971534729004, Discriminator Loss: 9.875791549682617\n",
      "Epoch 7, Generator Loss: -0.5121288895606995, Discriminator Loss: 9.873505592346191\n",
      "Epoch 7, Generator Loss: -0.5120799541473389, Discriminator Loss: 9.866654396057129\n",
      "Epoch 7, Generator Loss: -0.5127972960472107, Discriminator Loss: 9.86973762512207\n",
      "Epoch 7, Generator Loss: -0.5124583840370178, Discriminator Loss: 9.875210762023926\n",
      "Epoch 7, Generator Loss: -0.5124882459640503, Discriminator Loss: 9.872396469116211\n",
      "Epoch 7, Generator Loss: -0.5132958889007568, Discriminator Loss: 9.866179466247559\n",
      "Epoch 7, Generator Loss: -0.512631893157959, Discriminator Loss: 9.867685317993164\n",
      "Epoch 7, Generator Loss: -0.5134474039077759, Discriminator Loss: 9.860880851745605\n",
      "Epoch 7, Generator Loss: -0.5133163928985596, Discriminator Loss: 9.867461204528809\n",
      "Epoch 7, Generator Loss: -0.512765645980835, Discriminator Loss: 9.86149787902832\n",
      "Epoch 7, Generator Loss: -0.5134401321411133, Discriminator Loss: 9.862678527832031\n",
      "Epoch 7, Generator Loss: -0.5139240026473999, Discriminator Loss: 9.871302604675293\n",
      "Epoch 7, Generator Loss: -0.5140448808670044, Discriminator Loss: 9.861006736755371\n",
      "Epoch 7, Generator Loss: -0.5141364932060242, Discriminator Loss: 9.862630844116211\n",
      "Epoch 7, Generator Loss: -0.5132372379302979, Discriminator Loss: 9.861351013183594\n",
      "Epoch 7, Generator Loss: -0.5137462615966797, Discriminator Loss: 9.863110542297363\n",
      "Epoch 7, Generator Loss: -0.5138468742370605, Discriminator Loss: 9.854750633239746\n",
      "Epoch 7, Generator Loss: -0.514300525188446, Discriminator Loss: 9.864272117614746\n",
      "Epoch 7, Generator Loss: -0.5146374106407166, Discriminator Loss: 9.854890823364258\n",
      "Epoch 7, Generator Loss: -0.514728307723999, Discriminator Loss: 9.854090690612793\n",
      "Epoch 7, Generator Loss: -0.5140079259872437, Discriminator Loss: 9.861349105834961\n",
      "Epoch 7, Generator Loss: -0.5154550075531006, Discriminator Loss: 9.852066040039062\n",
      "Epoch 7, Generator Loss: -0.5145976543426514, Discriminator Loss: 9.859044075012207\n",
      "Epoch 7, Generator Loss: -0.5152602791786194, Discriminator Loss: 9.851853370666504\n",
      "Epoch 7, Generator Loss: -0.5156024694442749, Discriminator Loss: 9.858129501342773\n",
      "Epoch 7, Generator Loss: -0.51497882604599, Discriminator Loss: 9.853521347045898\n",
      "Epoch 7, Generator Loss: -0.5161895751953125, Discriminator Loss: 9.844864845275879\n",
      "Epoch 7, Generator Loss: -0.5158298015594482, Discriminator Loss: 9.853361129760742\n",
      "Epoch 7, Generator Loss: -0.5160784125328064, Discriminator Loss: 9.850065231323242\n",
      "Epoch 7, Generator Loss: -0.5163066387176514, Discriminator Loss: 9.840340614318848\n",
      "Epoch 7, Generator Loss: -0.516396164894104, Discriminator Loss: 9.84325885772705\n",
      "Epoch 7, Generator Loss: -0.5161951780319214, Discriminator Loss: 9.846264839172363\n",
      "Epoch 7, Generator Loss: -0.516110897064209, Discriminator Loss: 9.848359107971191\n",
      "Epoch 7, Generator Loss: -0.5167772769927979, Discriminator Loss: 9.84407901763916\n",
      "Epoch 7, Generator Loss: -0.5172030925750732, Discriminator Loss: 9.845491409301758\n",
      "Epoch 7, Generator Loss: -0.5170286893844604, Discriminator Loss: 9.851373672485352\n",
      "Epoch 7, Generator Loss: -0.5173496603965759, Discriminator Loss: 9.844265937805176\n",
      "Epoch 8, Generator Loss: -0.5179188251495361, Discriminator Loss: 9.838505744934082\n",
      "Epoch 8, Generator Loss: -0.5177028775215149, Discriminator Loss: 9.837516784667969\n",
      "Epoch 8, Generator Loss: -0.5175566077232361, Discriminator Loss: 9.837935447692871\n",
      "Epoch 8, Generator Loss: -0.5172391533851624, Discriminator Loss: 9.836464881896973\n",
      "Epoch 8, Generator Loss: -0.517763614654541, Discriminator Loss: 9.834156036376953\n",
      "Epoch 8, Generator Loss: -0.518517255783081, Discriminator Loss: 9.838113784790039\n",
      "Epoch 8, Generator Loss: -0.5186269283294678, Discriminator Loss: 9.834528923034668\n",
      "Epoch 8, Generator Loss: -0.5175873041152954, Discriminator Loss: 9.829540252685547\n",
      "Epoch 8, Generator Loss: -0.5185888409614563, Discriminator Loss: 9.841381072998047\n",
      "Epoch 8, Generator Loss: -0.5182527303695679, Discriminator Loss: 9.837587356567383\n",
      "Epoch 8, Generator Loss: -0.5184440612792969, Discriminator Loss: 9.83162784576416\n",
      "Epoch 8, Generator Loss: -0.5190668106079102, Discriminator Loss: 9.829141616821289\n",
      "Epoch 8, Generator Loss: -0.519194483757019, Discriminator Loss: 9.825749397277832\n",
      "Epoch 8, Generator Loss: -0.5193092823028564, Discriminator Loss: 9.825458526611328\n",
      "Epoch 8, Generator Loss: -0.518587052822113, Discriminator Loss: 9.828597068786621\n",
      "Epoch 8, Generator Loss: -0.5196417570114136, Discriminator Loss: 9.822272300720215\n",
      "Epoch 8, Generator Loss: -0.5200724601745605, Discriminator Loss: 9.820085525512695\n",
      "Epoch 8, Generator Loss: -0.5198547840118408, Discriminator Loss: 9.82278823852539\n",
      "Epoch 8, Generator Loss: -0.5199083089828491, Discriminator Loss: 9.81608772277832\n",
      "Epoch 8, Generator Loss: -0.5199300050735474, Discriminator Loss: 9.82699966430664\n",
      "Epoch 8, Generator Loss: -0.5207222700119019, Discriminator Loss: 9.82247543334961\n",
      "Epoch 8, Generator Loss: -0.5207732319831848, Discriminator Loss: 9.808296203613281\n",
      "Epoch 8, Generator Loss: -0.5207730531692505, Discriminator Loss: 9.82076358795166\n",
      "Epoch 8, Generator Loss: -0.5211212038993835, Discriminator Loss: 9.81245231628418\n",
      "Epoch 8, Generator Loss: -0.5205550789833069, Discriminator Loss: 9.816489219665527\n",
      "Epoch 8, Generator Loss: -0.5216159224510193, Discriminator Loss: 9.81778335571289\n",
      "Epoch 8, Generator Loss: -0.5214453935623169, Discriminator Loss: 9.816843032836914\n",
      "Epoch 8, Generator Loss: -0.5213073492050171, Discriminator Loss: 9.819517135620117\n",
      "Epoch 8, Generator Loss: -0.52152419090271, Discriminator Loss: 9.811630249023438\n",
      "Epoch 8, Generator Loss: -0.5227320194244385, Discriminator Loss: 9.810958862304688\n",
      "Epoch 8, Generator Loss: -0.5213120579719543, Discriminator Loss: 9.806520462036133\n",
      "Epoch 8, Generator Loss: -0.5217629671096802, Discriminator Loss: 9.808507919311523\n",
      "Epoch 8, Generator Loss: -0.5230036973953247, Discriminator Loss: 9.799267768859863\n",
      "Epoch 8, Generator Loss: -0.522222638130188, Discriminator Loss: 9.798322677612305\n",
      "Epoch 8, Generator Loss: -0.5222011804580688, Discriminator Loss: 9.821470260620117\n",
      "Epoch 8, Generator Loss: -0.522926390171051, Discriminator Loss: 9.80386734008789\n",
      "Epoch 8, Generator Loss: -0.5231091976165771, Discriminator Loss: 9.805373191833496\n",
      "Epoch 8, Generator Loss: -0.5235542058944702, Discriminator Loss: 9.792853355407715\n",
      "Epoch 8, Generator Loss: -0.5236425399780273, Discriminator Loss: 9.79166316986084\n",
      "Epoch 8, Generator Loss: -0.5235395431518555, Discriminator Loss: 9.798894882202148\n",
      "Epoch 8, Generator Loss: -0.523383617401123, Discriminator Loss: 9.795567512512207\n",
      "Epoch 8, Generator Loss: -0.5244817733764648, Discriminator Loss: 9.808557510375977\n",
      "Epoch 8, Generator Loss: -0.5236109495162964, Discriminator Loss: 9.80178451538086\n",
      "Epoch 8, Generator Loss: -0.5237517356872559, Discriminator Loss: 9.792734146118164\n",
      "Epoch 8, Generator Loss: -0.5238299369812012, Discriminator Loss: 9.801056861877441\n",
      "Epoch 8, Generator Loss: -0.5244767665863037, Discriminator Loss: 9.784773826599121\n",
      "Epoch 8, Generator Loss: -0.5243029594421387, Discriminator Loss: 9.787952423095703\n",
      "Epoch 8, Generator Loss: -0.525627613067627, Discriminator Loss: 9.78644847869873\n",
      "Epoch 8, Generator Loss: -0.5254476070404053, Discriminator Loss: 9.79008674621582\n",
      "Epoch 8, Generator Loss: -0.5253338813781738, Discriminator Loss: 9.78180980682373\n",
      "Epoch 9, Generator Loss: -0.526206910610199, Discriminator Loss: 9.797192573547363\n",
      "Epoch 9, Generator Loss: -0.525719404220581, Discriminator Loss: 9.788381576538086\n",
      "Epoch 9, Generator Loss: -0.5267705917358398, Discriminator Loss: 9.788779258728027\n",
      "Epoch 9, Generator Loss: -0.5261427164077759, Discriminator Loss: 9.79007625579834\n",
      "Epoch 9, Generator Loss: -0.5268361568450928, Discriminator Loss: 9.785786628723145\n",
      "Epoch 9, Generator Loss: -0.5269187688827515, Discriminator Loss: 9.784708023071289\n",
      "Epoch 9, Generator Loss: -0.526986837387085, Discriminator Loss: 9.784174919128418\n",
      "Epoch 9, Generator Loss: -0.5274089574813843, Discriminator Loss: 9.777169227600098\n",
      "Epoch 9, Generator Loss: -0.5254806280136108, Discriminator Loss: 9.775633811950684\n",
      "Epoch 9, Generator Loss: -0.526572048664093, Discriminator Loss: 9.777315139770508\n",
      "Epoch 9, Generator Loss: -0.5277915000915527, Discriminator Loss: 9.776833534240723\n",
      "Epoch 9, Generator Loss: -0.5267441272735596, Discriminator Loss: 9.760274887084961\n",
      "Epoch 9, Generator Loss: -0.5287173986434937, Discriminator Loss: 9.770066261291504\n",
      "Epoch 9, Generator Loss: -0.5294702053070068, Discriminator Loss: 9.757486343383789\n",
      "Epoch 9, Generator Loss: -0.5290145874023438, Discriminator Loss: 9.776346206665039\n",
      "Epoch 9, Generator Loss: -0.5281374454498291, Discriminator Loss: 9.772140502929688\n",
      "Epoch 9, Generator Loss: -0.5297040343284607, Discriminator Loss: 9.776451110839844\n",
      "Epoch 9, Generator Loss: -0.5305722951889038, Discriminator Loss: 9.766047477722168\n",
      "Epoch 9, Generator Loss: -0.5289047360420227, Discriminator Loss: 9.767037391662598\n",
      "Epoch 9, Generator Loss: -0.5297356843948364, Discriminator Loss: 9.751797676086426\n",
      "Epoch 9, Generator Loss: -0.5298359990119934, Discriminator Loss: 9.767401695251465\n",
      "Epoch 9, Generator Loss: -0.530386209487915, Discriminator Loss: 9.76382064819336\n",
      "Epoch 9, Generator Loss: -0.529777467250824, Discriminator Loss: 9.757865905761719\n",
      "Epoch 9, Generator Loss: -0.5321626663208008, Discriminator Loss: 9.7544527053833\n",
      "Epoch 9, Generator Loss: -0.5304288864135742, Discriminator Loss: 9.748449325561523\n",
      "Epoch 9, Generator Loss: -0.5315160155296326, Discriminator Loss: 9.76975154876709\n",
      "Epoch 9, Generator Loss: -0.5307084321975708, Discriminator Loss: 9.751967430114746\n",
      "Epoch 9, Generator Loss: -0.5318692922592163, Discriminator Loss: 9.748376846313477\n",
      "Epoch 9, Generator Loss: -0.5319951772689819, Discriminator Loss: 9.742752075195312\n",
      "Epoch 9, Generator Loss: -0.5326460599899292, Discriminator Loss: 9.750019073486328\n",
      "Epoch 9, Generator Loss: -0.5322540402412415, Discriminator Loss: 9.762785911560059\n",
      "Epoch 9, Generator Loss: -0.5329452157020569, Discriminator Loss: 9.751300811767578\n",
      "Epoch 9, Generator Loss: -0.5323315858840942, Discriminator Loss: 9.740738868713379\n",
      "Epoch 9, Generator Loss: -0.533768892288208, Discriminator Loss: 9.739734649658203\n",
      "Epoch 9, Generator Loss: -0.533007800579071, Discriminator Loss: 9.732112884521484\n",
      "Epoch 9, Generator Loss: -0.5349292755126953, Discriminator Loss: 9.735298156738281\n",
      "Epoch 9, Generator Loss: -0.5349605083465576, Discriminator Loss: 9.748846054077148\n",
      "Epoch 9, Generator Loss: -0.5320217609405518, Discriminator Loss: 9.742358207702637\n",
      "Epoch 9, Generator Loss: -0.5348232388496399, Discriminator Loss: 9.747162818908691\n",
      "Epoch 9, Generator Loss: -0.5331419110298157, Discriminator Loss: 9.72804069519043\n",
      "Epoch 9, Generator Loss: -0.5347000360488892, Discriminator Loss: 9.728017807006836\n",
      "Epoch 9, Generator Loss: -0.5331730842590332, Discriminator Loss: 9.734501838684082\n",
      "Epoch 9, Generator Loss: -0.5349302887916565, Discriminator Loss: 9.739850044250488\n",
      "Epoch 9, Generator Loss: -0.5371149778366089, Discriminator Loss: 9.734928131103516\n",
      "Epoch 9, Generator Loss: -0.5352405309677124, Discriminator Loss: 9.733672142028809\n",
      "Epoch 9, Generator Loss: -0.5355259776115417, Discriminator Loss: 9.731524467468262\n",
      "Epoch 9, Generator Loss: -0.5361933708190918, Discriminator Loss: 9.71669864654541\n",
      "Epoch 9, Generator Loss: -0.5359289646148682, Discriminator Loss: 9.718314170837402\n",
      "Epoch 9, Generator Loss: -0.5363878011703491, Discriminator Loss: 9.731001853942871\n",
      "Epoch 9, Generator Loss: -0.5359829664230347, Discriminator Loss: 9.714953422546387\n",
      "Epoch 10, Generator Loss: -0.5365340113639832, Discriminator Loss: 9.721755981445312\n",
      "Epoch 10, Generator Loss: -0.537987232208252, Discriminator Loss: 9.71557903289795\n",
      "Epoch 10, Generator Loss: -0.5367460250854492, Discriminator Loss: 9.717418670654297\n",
      "Epoch 10, Generator Loss: -0.5375548601150513, Discriminator Loss: 9.707658767700195\n",
      "Epoch 10, Generator Loss: -0.5390284061431885, Discriminator Loss: 9.72016716003418\n",
      "Epoch 10, Generator Loss: -0.5382511019706726, Discriminator Loss: 9.72060775756836\n",
      "Epoch 10, Generator Loss: -0.5396747589111328, Discriminator Loss: 9.711284637451172\n",
      "Epoch 10, Generator Loss: -0.5400217771530151, Discriminator Loss: 9.702474594116211\n",
      "Epoch 10, Generator Loss: -0.539497971534729, Discriminator Loss: 9.709761619567871\n",
      "Epoch 10, Generator Loss: -0.538483202457428, Discriminator Loss: 9.706247329711914\n",
      "Epoch 10, Generator Loss: -0.5394904613494873, Discriminator Loss: 9.701632499694824\n",
      "Epoch 10, Generator Loss: -0.5404055714607239, Discriminator Loss: 9.702214241027832\n",
      "Epoch 10, Generator Loss: -0.5396578907966614, Discriminator Loss: 9.697269439697266\n",
      "Epoch 10, Generator Loss: -0.539647102355957, Discriminator Loss: 9.705976486206055\n",
      "Epoch 10, Generator Loss: -0.5399703979492188, Discriminator Loss: 9.689184188842773\n",
      "Epoch 10, Generator Loss: -0.5422106385231018, Discriminator Loss: 9.709050178527832\n",
      "Epoch 10, Generator Loss: -0.5403679609298706, Discriminator Loss: 9.7087984085083\n",
      "Epoch 10, Generator Loss: -0.5426213145256042, Discriminator Loss: 9.678882598876953\n",
      "Epoch 10, Generator Loss: -0.5429967641830444, Discriminator Loss: 9.700082778930664\n",
      "Epoch 10, Generator Loss: -0.5430796146392822, Discriminator Loss: 9.6904935836792\n",
      "Epoch 10, Generator Loss: -0.5423527956008911, Discriminator Loss: 9.701677322387695\n",
      "Epoch 10, Generator Loss: -0.5426703691482544, Discriminator Loss: 9.693750381469727\n",
      "Epoch 10, Generator Loss: -0.542603611946106, Discriminator Loss: 9.685805320739746\n",
      "Epoch 10, Generator Loss: -0.543979287147522, Discriminator Loss: 9.691305160522461\n",
      "Epoch 10, Generator Loss: -0.5436983108520508, Discriminator Loss: 9.682324409484863\n",
      "Epoch 10, Generator Loss: -0.5434242486953735, Discriminator Loss: 9.67475414276123\n",
      "Epoch 10, Generator Loss: -0.543571949005127, Discriminator Loss: 9.667716979980469\n",
      "Epoch 10, Generator Loss: -0.5460144281387329, Discriminator Loss: 9.687621116638184\n",
      "Epoch 10, Generator Loss: -0.5439735054969788, Discriminator Loss: 9.679875373840332\n",
      "Epoch 10, Generator Loss: -0.5450863838195801, Discriminator Loss: 9.680261611938477\n",
      "Epoch 10, Generator Loss: -0.5478814840316772, Discriminator Loss: 9.679338455200195\n",
      "Epoch 10, Generator Loss: -0.5449654459953308, Discriminator Loss: 9.668462753295898\n",
      "Epoch 10, Generator Loss: -0.5462675094604492, Discriminator Loss: 9.688092231750488\n",
      "Epoch 10, Generator Loss: -0.5474038124084473, Discriminator Loss: 9.665451049804688\n",
      "Epoch 10, Generator Loss: -0.5457327365875244, Discriminator Loss: 9.662874221801758\n",
      "Epoch 10, Generator Loss: -0.5473198294639587, Discriminator Loss: 9.659440994262695\n",
      "Epoch 10, Generator Loss: -0.5470215678215027, Discriminator Loss: 9.650579452514648\n",
      "Epoch 10, Generator Loss: -0.5470263957977295, Discriminator Loss: 9.657672882080078\n",
      "Epoch 10, Generator Loss: -0.5472571849822998, Discriminator Loss: 9.646378517150879\n",
      "Epoch 10, Generator Loss: -0.547928512096405, Discriminator Loss: 9.651338577270508\n",
      "Epoch 10, Generator Loss: -0.5500552654266357, Discriminator Loss: 9.660958290100098\n",
      "Epoch 10, Generator Loss: -0.5503913760185242, Discriminator Loss: 9.653677940368652\n",
      "Epoch 10, Generator Loss: -0.5479035973548889, Discriminator Loss: 9.656177520751953\n",
      "Epoch 10, Generator Loss: -0.5491877794265747, Discriminator Loss: 9.64620590209961\n",
      "Epoch 10, Generator Loss: -0.5501255393028259, Discriminator Loss: 9.642648696899414\n",
      "Epoch 10, Generator Loss: -0.5507071614265442, Discriminator Loss: 9.66367244720459\n",
      "Epoch 10, Generator Loss: -0.5493576526641846, Discriminator Loss: 9.649291038513184\n",
      "Epoch 10, Generator Loss: -0.5507867336273193, Discriminator Loss: 9.652043342590332\n",
      "Epoch 10, Generator Loss: -0.5509555339813232, Discriminator Loss: 9.65134334564209\n",
      "Epoch 10, Generator Loss: -0.5537014603614807, Discriminator Loss: 9.635820388793945\n",
      "Epoch 11, Generator Loss: -0.5507264137268066, Discriminator Loss: 9.646286964416504\n",
      "Epoch 11, Generator Loss: -0.5532916784286499, Discriminator Loss: 9.644057273864746\n",
      "Epoch 11, Generator Loss: -0.5528059005737305, Discriminator Loss: 9.636226654052734\n",
      "Epoch 11, Generator Loss: -0.5538170337677002, Discriminator Loss: 9.63369083404541\n",
      "Epoch 11, Generator Loss: -0.5520069599151611, Discriminator Loss: 9.63846206665039\n",
      "Epoch 11, Generator Loss: -0.5535808205604553, Discriminator Loss: 9.629035949707031\n",
      "Epoch 11, Generator Loss: -0.5535247921943665, Discriminator Loss: 9.631176948547363\n",
      "Epoch 11, Generator Loss: -0.554431676864624, Discriminator Loss: 9.601781845092773\n",
      "Epoch 11, Generator Loss: -0.554692804813385, Discriminator Loss: 9.62832260131836\n",
      "Epoch 11, Generator Loss: -0.5548390746116638, Discriminator Loss: 9.616765975952148\n",
      "Epoch 11, Generator Loss: -0.5550814270973206, Discriminator Loss: 9.625027656555176\n",
      "Epoch 11, Generator Loss: -0.5544583797454834, Discriminator Loss: 9.610194206237793\n",
      "Epoch 11, Generator Loss: -0.5571228265762329, Discriminator Loss: 9.628792762756348\n",
      "Epoch 11, Generator Loss: -0.556430459022522, Discriminator Loss: 9.627705574035645\n",
      "Epoch 11, Generator Loss: -0.556228518486023, Discriminator Loss: 9.616079330444336\n",
      "Epoch 11, Generator Loss: -0.5569712519645691, Discriminator Loss: 9.609705924987793\n",
      "Epoch 11, Generator Loss: -0.5560563206672668, Discriminator Loss: 9.60910415649414\n",
      "Epoch 11, Generator Loss: -0.5571234226226807, Discriminator Loss: 9.61153507232666\n",
      "Epoch 11, Generator Loss: -0.559137225151062, Discriminator Loss: 9.609658241271973\n",
      "Epoch 11, Generator Loss: -0.5576562881469727, Discriminator Loss: 9.596060752868652\n",
      "Epoch 11, Generator Loss: -0.5581233501434326, Discriminator Loss: 9.60523796081543\n",
      "Epoch 11, Generator Loss: -0.5624065399169922, Discriminator Loss: 9.601386070251465\n",
      "Epoch 11, Generator Loss: -0.5576428174972534, Discriminator Loss: 9.61538028717041\n",
      "Epoch 11, Generator Loss: -0.5585220456123352, Discriminator Loss: 9.601789474487305\n",
      "Epoch 11, Generator Loss: -0.5594464540481567, Discriminator Loss: 9.604240417480469\n",
      "Epoch 11, Generator Loss: -0.5614268779754639, Discriminator Loss: 9.597553253173828\n",
      "Epoch 11, Generator Loss: -0.5615919232368469, Discriminator Loss: 9.587477684020996\n",
      "Epoch 11, Generator Loss: -0.5606624484062195, Discriminator Loss: 9.592761993408203\n",
      "Epoch 11, Generator Loss: -0.5637369155883789, Discriminator Loss: 9.583497047424316\n",
      "Epoch 11, Generator Loss: -0.5604769587516785, Discriminator Loss: 9.590716361999512\n",
      "Epoch 11, Generator Loss: -0.5596188306808472, Discriminator Loss: 9.592909812927246\n",
      "Epoch 11, Generator Loss: -0.5627691745758057, Discriminator Loss: 9.591952323913574\n",
      "Epoch 11, Generator Loss: -0.5621054172515869, Discriminator Loss: 9.603422164916992\n",
      "Epoch 11, Generator Loss: -0.5622913837432861, Discriminator Loss: 9.581995964050293\n",
      "Epoch 11, Generator Loss: -0.5633375644683838, Discriminator Loss: 9.573331832885742\n",
      "Epoch 11, Generator Loss: -0.5659278631210327, Discriminator Loss: 9.587671279907227\n",
      "Epoch 11, Generator Loss: -0.5661067962646484, Discriminator Loss: 9.570408821105957\n",
      "Epoch 11, Generator Loss: -0.5644886493682861, Discriminator Loss: 9.566412925720215\n",
      "Epoch 11, Generator Loss: -0.5659937262535095, Discriminator Loss: 9.573713302612305\n",
      "Epoch 11, Generator Loss: -0.5668773651123047, Discriminator Loss: 9.586967468261719\n",
      "Epoch 11, Generator Loss: -0.567484438419342, Discriminator Loss: 9.563803672790527\n",
      "Epoch 11, Generator Loss: -0.5652775764465332, Discriminator Loss: 9.558965682983398\n",
      "Epoch 11, Generator Loss: -0.567857563495636, Discriminator Loss: 9.558602333068848\n",
      "Epoch 11, Generator Loss: -0.5682214498519897, Discriminator Loss: 9.557511329650879\n",
      "Epoch 11, Generator Loss: -0.565692663192749, Discriminator Loss: 9.55474853515625\n",
      "Epoch 11, Generator Loss: -0.5671848058700562, Discriminator Loss: 9.557960510253906\n",
      "Epoch 11, Generator Loss: -0.5680962800979614, Discriminator Loss: 9.544230461120605\n",
      "Epoch 11, Generator Loss: -0.5681661367416382, Discriminator Loss: 9.56688404083252\n",
      "Epoch 11, Generator Loss: -0.5686806440353394, Discriminator Loss: 9.574933052062988\n",
      "Epoch 11, Generator Loss: -0.5713799595832825, Discriminator Loss: 9.551044464111328\n",
      "Epoch 12, Generator Loss: -0.5712640285491943, Discriminator Loss: 9.57442569732666\n",
      "Epoch 12, Generator Loss: -0.5713918209075928, Discriminator Loss: 9.544595718383789\n",
      "Epoch 12, Generator Loss: -0.5714319348335266, Discriminator Loss: 9.528494834899902\n",
      "Epoch 12, Generator Loss: -0.5708717107772827, Discriminator Loss: 9.5348482131958\n",
      "Epoch 12, Generator Loss: -0.5719372034072876, Discriminator Loss: 9.548004150390625\n",
      "Epoch 12, Generator Loss: -0.5731145739555359, Discriminator Loss: 9.538797378540039\n",
      "Epoch 12, Generator Loss: -0.5698598027229309, Discriminator Loss: 9.545049667358398\n",
      "Epoch 12, Generator Loss: -0.575808584690094, Discriminator Loss: 9.524911880493164\n",
      "Epoch 12, Generator Loss: -0.5726428031921387, Discriminator Loss: 9.552505493164062\n",
      "Epoch 12, Generator Loss: -0.5747048854827881, Discriminator Loss: 9.527597427368164\n",
      "Epoch 12, Generator Loss: -0.5743693113327026, Discriminator Loss: 9.53822135925293\n",
      "Epoch 12, Generator Loss: -0.5755943655967712, Discriminator Loss: 9.536584854125977\n",
      "Epoch 12, Generator Loss: -0.5745911002159119, Discriminator Loss: 9.52397632598877\n",
      "Epoch 12, Generator Loss: -0.5744394659996033, Discriminator Loss: 9.535340309143066\n",
      "Epoch 12, Generator Loss: -0.5751901865005493, Discriminator Loss: 9.527446746826172\n",
      "Epoch 12, Generator Loss: -0.5772191882133484, Discriminator Loss: 9.525250434875488\n",
      "Epoch 12, Generator Loss: -0.5754467844963074, Discriminator Loss: 9.52734088897705\n",
      "Epoch 12, Generator Loss: -0.5763195157051086, Discriminator Loss: 9.524124145507812\n",
      "Epoch 12, Generator Loss: -0.5750291347503662, Discriminator Loss: 9.515159606933594\n",
      "Epoch 12, Generator Loss: -0.5791088342666626, Discriminator Loss: 9.51253604888916\n",
      "Epoch 12, Generator Loss: -0.5785659551620483, Discriminator Loss: 9.50563907623291\n",
      "Epoch 12, Generator Loss: -0.5802484154701233, Discriminator Loss: 9.511030197143555\n",
      "Epoch 12, Generator Loss: -0.5799367427825928, Discriminator Loss: 9.509119033813477\n",
      "Epoch 12, Generator Loss: -0.5797486305236816, Discriminator Loss: 9.510382652282715\n",
      "Epoch 12, Generator Loss: -0.5761763453483582, Discriminator Loss: 9.490251541137695\n",
      "Epoch 12, Generator Loss: -0.5810329914093018, Discriminator Loss: 9.50780200958252\n",
      "Epoch 12, Generator Loss: -0.5816004872322083, Discriminator Loss: 9.524158477783203\n",
      "Epoch 12, Generator Loss: -0.5815348625183105, Discriminator Loss: 9.48954963684082\n",
      "Epoch 12, Generator Loss: -0.5829681754112244, Discriminator Loss: 9.487327575683594\n",
      "Epoch 12, Generator Loss: -0.5831589698791504, Discriminator Loss: 9.492263793945312\n",
      "Epoch 12, Generator Loss: -0.5832970142364502, Discriminator Loss: 9.487884521484375\n",
      "Epoch 12, Generator Loss: -0.5860571265220642, Discriminator Loss: 9.48820972442627\n",
      "Epoch 12, Generator Loss: -0.5860511660575867, Discriminator Loss: 9.516862869262695\n",
      "Epoch 12, Generator Loss: -0.5838379263877869, Discriminator Loss: 9.486957550048828\n",
      "Epoch 12, Generator Loss: -0.5859111547470093, Discriminator Loss: 9.495280265808105\n",
      "Epoch 12, Generator Loss: -0.5832592248916626, Discriminator Loss: 9.460700035095215\n",
      "Epoch 12, Generator Loss: -0.5823084115982056, Discriminator Loss: 9.466614723205566\n",
      "Epoch 12, Generator Loss: -0.5831434726715088, Discriminator Loss: 9.485814094543457\n",
      "Epoch 12, Generator Loss: -0.5896762609481812, Discriminator Loss: 9.476099014282227\n",
      "Epoch 12, Generator Loss: -0.5866327881813049, Discriminator Loss: 9.474834442138672\n",
      "Epoch 12, Generator Loss: -0.5861644744873047, Discriminator Loss: 9.461248397827148\n",
      "Epoch 12, Generator Loss: -0.5887274146080017, Discriminator Loss: 9.47183895111084\n",
      "Epoch 12, Generator Loss: -0.5876229405403137, Discriminator Loss: 9.454513549804688\n",
      "Epoch 12, Generator Loss: -0.584856390953064, Discriminator Loss: 9.425046920776367\n",
      "Epoch 12, Generator Loss: -0.5880670547485352, Discriminator Loss: 9.478961944580078\n",
      "Epoch 12, Generator Loss: -0.5856629610061646, Discriminator Loss: 9.459074974060059\n",
      "Epoch 12, Generator Loss: -0.5923901200294495, Discriminator Loss: 9.447405815124512\n",
      "Epoch 12, Generator Loss: -0.5923861265182495, Discriminator Loss: 9.438241004943848\n",
      "Epoch 12, Generator Loss: -0.5894758105278015, Discriminator Loss: 9.437381744384766\n",
      "Epoch 12, Generator Loss: -0.5917773842811584, Discriminator Loss: 9.437169075012207\n",
      "Epoch 13, Generator Loss: -0.5909563302993774, Discriminator Loss: 9.425321578979492\n",
      "Epoch 13, Generator Loss: -0.5915403366088867, Discriminator Loss: 9.439558982849121\n",
      "Epoch 13, Generator Loss: -0.5940049886703491, Discriminator Loss: 9.450762748718262\n",
      "Epoch 13, Generator Loss: -0.5958871841430664, Discriminator Loss: 9.451172828674316\n",
      "Epoch 13, Generator Loss: -0.5924021005630493, Discriminator Loss: 9.470799446105957\n",
      "Epoch 13, Generator Loss: -0.5949217081069946, Discriminator Loss: 9.424832344055176\n",
      "Epoch 13, Generator Loss: -0.5963262319564819, Discriminator Loss: 9.424407958984375\n",
      "Epoch 13, Generator Loss: -0.5931381583213806, Discriminator Loss: 9.416740417480469\n",
      "Epoch 13, Generator Loss: -0.5972651243209839, Discriminator Loss: 9.454244613647461\n",
      "Epoch 13, Generator Loss: -0.5983966588973999, Discriminator Loss: 9.428975105285645\n",
      "Epoch 13, Generator Loss: -0.5952573418617249, Discriminator Loss: 9.411375999450684\n",
      "Epoch 13, Generator Loss: -0.5967119932174683, Discriminator Loss: 9.392131805419922\n",
      "Epoch 13, Generator Loss: -0.5976988077163696, Discriminator Loss: 9.414407730102539\n",
      "Epoch 13, Generator Loss: -0.5981156229972839, Discriminator Loss: 9.398427963256836\n",
      "Epoch 13, Generator Loss: -0.6020816564559937, Discriminator Loss: 9.403860092163086\n",
      "Epoch 13, Generator Loss: -0.5983102321624756, Discriminator Loss: 9.403082847595215\n",
      "Epoch 13, Generator Loss: -0.5993634462356567, Discriminator Loss: 9.39676284790039\n",
      "Epoch 13, Generator Loss: -0.5995292663574219, Discriminator Loss: 9.3944673538208\n",
      "Epoch 13, Generator Loss: -0.6006410717964172, Discriminator Loss: 9.409210205078125\n",
      "Epoch 13, Generator Loss: -0.6032893657684326, Discriminator Loss: 9.37816047668457\n",
      "Epoch 13, Generator Loss: -0.6012142896652222, Discriminator Loss: 9.408976554870605\n",
      "Epoch 13, Generator Loss: -0.6034599542617798, Discriminator Loss: 9.420543670654297\n",
      "Epoch 13, Generator Loss: -0.6044740676879883, Discriminator Loss: 9.391389846801758\n",
      "Epoch 13, Generator Loss: -0.6052662134170532, Discriminator Loss: 9.37025260925293\n",
      "Epoch 13, Generator Loss: -0.6008084416389465, Discriminator Loss: 9.363334655761719\n",
      "Epoch 13, Generator Loss: -0.6040799617767334, Discriminator Loss: 9.403156280517578\n",
      "Epoch 13, Generator Loss: -0.6052104234695435, Discriminator Loss: 9.408544540405273\n",
      "Epoch 13, Generator Loss: -0.6004800200462341, Discriminator Loss: 9.336175918579102\n",
      "Epoch 13, Generator Loss: -0.608440637588501, Discriminator Loss: 9.390673637390137\n",
      "Epoch 13, Generator Loss: -0.6057037115097046, Discriminator Loss: 9.395161628723145\n",
      "Epoch 13, Generator Loss: -0.604478120803833, Discriminator Loss: 9.371777534484863\n",
      "Epoch 13, Generator Loss: -0.6078064441680908, Discriminator Loss: 9.385144233703613\n",
      "Epoch 13, Generator Loss: -0.608790397644043, Discriminator Loss: 9.396319389343262\n",
      "Epoch 13, Generator Loss: -0.6062155961990356, Discriminator Loss: 9.379387855529785\n",
      "Epoch 13, Generator Loss: -0.6071619987487793, Discriminator Loss: 9.371089935302734\n",
      "Epoch 13, Generator Loss: -0.608293890953064, Discriminator Loss: 9.375530242919922\n",
      "Epoch 13, Generator Loss: -0.6082483530044556, Discriminator Loss: 9.354863166809082\n",
      "Epoch 13, Generator Loss: -0.6119362711906433, Discriminator Loss: 9.37463665008545\n",
      "Epoch 13, Generator Loss: -0.6091443300247192, Discriminator Loss: 9.366403579711914\n",
      "Epoch 13, Generator Loss: -0.6146988272666931, Discriminator Loss: 9.365700721740723\n",
      "Epoch 13, Generator Loss: -0.6135154366493225, Discriminator Loss: 9.357461929321289\n",
      "Epoch 13, Generator Loss: -0.6126506328582764, Discriminator Loss: 9.354660034179688\n",
      "Epoch 13, Generator Loss: -0.614254355430603, Discriminator Loss: 9.356988906860352\n",
      "Epoch 13, Generator Loss: -0.6157143712043762, Discriminator Loss: 9.347061157226562\n",
      "Epoch 13, Generator Loss: -0.6125026941299438, Discriminator Loss: 9.317366600036621\n",
      "Epoch 13, Generator Loss: -0.612473726272583, Discriminator Loss: 9.340024948120117\n",
      "Epoch 13, Generator Loss: -0.6156113147735596, Discriminator Loss: 9.358942031860352\n",
      "Epoch 13, Generator Loss: -0.6177354454994202, Discriminator Loss: 9.338942527770996\n",
      "Epoch 13, Generator Loss: -0.620378851890564, Discriminator Loss: 9.344234466552734\n",
      "Epoch 13, Generator Loss: -0.6140642762184143, Discriminator Loss: 9.320971488952637\n",
      "Epoch 14, Generator Loss: -0.6167846918106079, Discriminator Loss: 9.315751075744629\n",
      "Epoch 14, Generator Loss: -0.6178361177444458, Discriminator Loss: 9.30294132232666\n",
      "Epoch 14, Generator Loss: -0.6214237809181213, Discriminator Loss: 9.34752368927002\n",
      "Epoch 14, Generator Loss: -0.6203324794769287, Discriminator Loss: 9.327329635620117\n",
      "Epoch 14, Generator Loss: -0.6226342916488647, Discriminator Loss: 9.298358917236328\n",
      "Epoch 14, Generator Loss: -0.6187677383422852, Discriminator Loss: 9.327003479003906\n",
      "Epoch 14, Generator Loss: -0.6194623708724976, Discriminator Loss: 9.301898956298828\n",
      "Epoch 14, Generator Loss: -0.6214778423309326, Discriminator Loss: 9.310782432556152\n",
      "Epoch 14, Generator Loss: -0.6204800605773926, Discriminator Loss: 9.306072235107422\n",
      "Epoch 14, Generator Loss: -0.6161907911300659, Discriminator Loss: 9.298187255859375\n",
      "Epoch 14, Generator Loss: -0.6257249116897583, Discriminator Loss: 9.329957962036133\n",
      "Epoch 14, Generator Loss: -0.624104380607605, Discriminator Loss: 9.291152954101562\n",
      "Epoch 14, Generator Loss: -0.6254495978355408, Discriminator Loss: 9.307502746582031\n",
      "Epoch 14, Generator Loss: -0.6196106672286987, Discriminator Loss: 9.317976951599121\n",
      "Epoch 14, Generator Loss: -0.6245983839035034, Discriminator Loss: 9.265413284301758\n",
      "Epoch 14, Generator Loss: -0.6235240697860718, Discriminator Loss: 9.288190841674805\n",
      "Epoch 14, Generator Loss: -0.6328479051589966, Discriminator Loss: 9.317207336425781\n",
      "Epoch 14, Generator Loss: -0.6293310523033142, Discriminator Loss: 9.265625\n",
      "Epoch 14, Generator Loss: -0.623744010925293, Discriminator Loss: 9.270953178405762\n",
      "Epoch 14, Generator Loss: -0.6290024518966675, Discriminator Loss: 9.284859657287598\n",
      "Epoch 14, Generator Loss: -0.6287280917167664, Discriminator Loss: 9.253021240234375\n",
      "Epoch 14, Generator Loss: -0.6313775777816772, Discriminator Loss: 9.261226654052734\n",
      "Epoch 14, Generator Loss: -0.6337664127349854, Discriminator Loss: 9.25456714630127\n",
      "Epoch 14, Generator Loss: -0.6296840906143188, Discriminator Loss: 9.259590148925781\n",
      "Epoch 14, Generator Loss: -0.6282342076301575, Discriminator Loss: 9.251531600952148\n",
      "Epoch 14, Generator Loss: -0.6288032531738281, Discriminator Loss: 9.271320343017578\n",
      "Epoch 14, Generator Loss: -0.6274901032447815, Discriminator Loss: 9.248199462890625\n",
      "Epoch 14, Generator Loss: -0.6315189003944397, Discriminator Loss: 9.265567779541016\n",
      "Epoch 14, Generator Loss: -0.6302467584609985, Discriminator Loss: 9.279519081115723\n",
      "Epoch 14, Generator Loss: -0.6352218389511108, Discriminator Loss: 9.247254371643066\n",
      "Epoch 14, Generator Loss: -0.6371080279350281, Discriminator Loss: 9.2553129196167\n",
      "Epoch 14, Generator Loss: -0.6349033117294312, Discriminator Loss: 9.236862182617188\n",
      "Epoch 14, Generator Loss: -0.6344321966171265, Discriminator Loss: 9.258008003234863\n",
      "Epoch 14, Generator Loss: -0.6375330090522766, Discriminator Loss: 9.25752067565918\n",
      "Epoch 14, Generator Loss: -0.6356407999992371, Discriminator Loss: 9.243356704711914\n",
      "Epoch 14, Generator Loss: -0.6385643482208252, Discriminator Loss: 9.211143493652344\n",
      "Epoch 14, Generator Loss: -0.6358223557472229, Discriminator Loss: 9.233085632324219\n",
      "Epoch 14, Generator Loss: -0.6405775547027588, Discriminator Loss: 9.244453430175781\n",
      "Epoch 14, Generator Loss: -0.6371692419052124, Discriminator Loss: 9.214951515197754\n",
      "Epoch 14, Generator Loss: -0.6425049304962158, Discriminator Loss: 9.248844146728516\n",
      "Epoch 14, Generator Loss: -0.6370155811309814, Discriminator Loss: 9.203566551208496\n",
      "Epoch 14, Generator Loss: -0.6363495588302612, Discriminator Loss: 9.21146011352539\n",
      "Epoch 14, Generator Loss: -0.6398748159408569, Discriminator Loss: 9.212367057800293\n",
      "Epoch 14, Generator Loss: -0.6407135128974915, Discriminator Loss: 9.180598258972168\n",
      "Epoch 14, Generator Loss: -0.6454676389694214, Discriminator Loss: 9.208860397338867\n",
      "Epoch 14, Generator Loss: -0.640845775604248, Discriminator Loss: 9.207181930541992\n",
      "Epoch 14, Generator Loss: -0.6456155776977539, Discriminator Loss: 9.177963256835938\n",
      "Epoch 14, Generator Loss: -0.644355058670044, Discriminator Loss: 9.21243953704834\n",
      "Epoch 14, Generator Loss: -0.6460526585578918, Discriminator Loss: 9.182628631591797\n",
      "Epoch 14, Generator Loss: -0.6409293413162231, Discriminator Loss: 9.221007347106934\n",
      "Epoch 15, Generator Loss: -0.643035888671875, Discriminator Loss: 9.203646659851074\n",
      "Epoch 15, Generator Loss: -0.6458851099014282, Discriminator Loss: 9.18539047241211\n",
      "Epoch 15, Generator Loss: -0.6484493017196655, Discriminator Loss: 9.188796997070312\n",
      "Epoch 15, Generator Loss: -0.652112603187561, Discriminator Loss: 9.182154655456543\n",
      "Epoch 15, Generator Loss: -0.6424994468688965, Discriminator Loss: 9.155193328857422\n",
      "Epoch 15, Generator Loss: -0.6487059593200684, Discriminator Loss: 9.174845695495605\n",
      "Epoch 15, Generator Loss: -0.6508719325065613, Discriminator Loss: 9.163437843322754\n",
      "Epoch 15, Generator Loss: -0.6485270261764526, Discriminator Loss: 9.141080856323242\n",
      "Epoch 15, Generator Loss: -0.6486780643463135, Discriminator Loss: 9.163728713989258\n",
      "Epoch 15, Generator Loss: -0.6533432006835938, Discriminator Loss: 9.1436767578125\n",
      "Epoch 15, Generator Loss: -0.6536877155303955, Discriminator Loss: 9.16760540008545\n",
      "Epoch 15, Generator Loss: -0.6497012376785278, Discriminator Loss: 9.190386772155762\n",
      "Epoch 15, Generator Loss: -0.6519229412078857, Discriminator Loss: 9.167715072631836\n",
      "Epoch 15, Generator Loss: -0.6520131826400757, Discriminator Loss: 9.134527206420898\n",
      "Epoch 15, Generator Loss: -0.6499290466308594, Discriminator Loss: 9.152945518493652\n",
      "Epoch 15, Generator Loss: -0.657607913017273, Discriminator Loss: 9.132935523986816\n",
      "Epoch 15, Generator Loss: -0.6576136350631714, Discriminator Loss: 9.159839630126953\n",
      "Epoch 15, Generator Loss: -0.6590492725372314, Discriminator Loss: 9.146981239318848\n",
      "Epoch 15, Generator Loss: -0.6580498218536377, Discriminator Loss: 9.11004638671875\n",
      "Epoch 15, Generator Loss: -0.6581603288650513, Discriminator Loss: 9.14505672454834\n",
      "Epoch 15, Generator Loss: -0.6590320467948914, Discriminator Loss: 9.131108283996582\n",
      "Epoch 15, Generator Loss: -0.6604150533676147, Discriminator Loss: 9.132023811340332\n",
      "Epoch 15, Generator Loss: -0.6566081047058105, Discriminator Loss: 9.135498046875\n",
      "Epoch 15, Generator Loss: -0.6636191606521606, Discriminator Loss: 9.126328468322754\n",
      "Epoch 15, Generator Loss: -0.6601613759994507, Discriminator Loss: 9.112702369689941\n",
      "Epoch 15, Generator Loss: -0.6600568294525146, Discriminator Loss: 9.125190734863281\n",
      "Epoch 15, Generator Loss: -0.6606400012969971, Discriminator Loss: 9.130311012268066\n",
      "Epoch 15, Generator Loss: -0.662456214427948, Discriminator Loss: 9.103728294372559\n",
      "Epoch 15, Generator Loss: -0.6641706228256226, Discriminator Loss: 9.127928733825684\n",
      "Epoch 15, Generator Loss: -0.6640295386314392, Discriminator Loss: 9.107889175415039\n",
      "Epoch 15, Generator Loss: -0.6626267433166504, Discriminator Loss: 9.128904342651367\n",
      "Epoch 15, Generator Loss: -0.6710029244422913, Discriminator Loss: 9.108617782592773\n",
      "Epoch 15, Generator Loss: -0.6665012240409851, Discriminator Loss: 9.086140632629395\n",
      "Epoch 15, Generator Loss: -0.6682168245315552, Discriminator Loss: 9.103133201599121\n",
      "Epoch 15, Generator Loss: -0.6660685539245605, Discriminator Loss: 9.12442684173584\n",
      "Epoch 15, Generator Loss: -0.667855978012085, Discriminator Loss: 9.11433219909668\n",
      "Epoch 15, Generator Loss: -0.6656885147094727, Discriminator Loss: 9.129425048828125\n",
      "Epoch 15, Generator Loss: -0.6756007075309753, Discriminator Loss: 9.103690147399902\n",
      "Epoch 15, Generator Loss: -0.6721150279045105, Discriminator Loss: 9.087841033935547\n",
      "Epoch 15, Generator Loss: -0.6816924214363098, Discriminator Loss: 9.107331275939941\n",
      "Epoch 15, Generator Loss: -0.6723624467849731, Discriminator Loss: 9.097569465637207\n",
      "Epoch 15, Generator Loss: -0.6706315875053406, Discriminator Loss: 9.041840553283691\n",
      "Epoch 15, Generator Loss: -0.675230085849762, Discriminator Loss: 9.04189395904541\n",
      "Epoch 15, Generator Loss: -0.6742586493492126, Discriminator Loss: 9.066755294799805\n",
      "Epoch 15, Generator Loss: -0.6822407245635986, Discriminator Loss: 9.049640655517578\n",
      "Epoch 15, Generator Loss: -0.6749200820922852, Discriminator Loss: 9.062009811401367\n",
      "Epoch 15, Generator Loss: -0.6825661659240723, Discriminator Loss: 9.071211814880371\n",
      "Epoch 15, Generator Loss: -0.6802600622177124, Discriminator Loss: 9.037551879882812\n",
      "Epoch 15, Generator Loss: -0.6781781315803528, Discriminator Loss: 9.08308219909668\n",
      "Epoch 15, Generator Loss: -0.679091215133667, Discriminator Loss: 9.086196899414062\n",
      "Epoch 16, Generator Loss: -0.673633873462677, Discriminator Loss: 8.996237754821777\n",
      "Epoch 16, Generator Loss: -0.6766268014907837, Discriminator Loss: 9.061654090881348\n",
      "Epoch 16, Generator Loss: -0.6823134422302246, Discriminator Loss: 9.027811050415039\n",
      "Epoch 16, Generator Loss: -0.6782176494598389, Discriminator Loss: 9.043168067932129\n",
      "Epoch 16, Generator Loss: -0.6833043694496155, Discriminator Loss: 9.027477264404297\n",
      "Epoch 16, Generator Loss: -0.6821709871292114, Discriminator Loss: 9.02939510345459\n",
      "Epoch 16, Generator Loss: -0.6807883381843567, Discriminator Loss: 9.046361923217773\n",
      "Epoch 16, Generator Loss: -0.6862505078315735, Discriminator Loss: 9.042909622192383\n",
      "Epoch 16, Generator Loss: -0.6805158257484436, Discriminator Loss: 9.019660949707031\n",
      "Epoch 16, Generator Loss: -0.6827155947685242, Discriminator Loss: 9.029598236083984\n",
      "Epoch 16, Generator Loss: -0.6923282146453857, Discriminator Loss: 9.04023551940918\n",
      "Epoch 16, Generator Loss: -0.6850569248199463, Discriminator Loss: 8.9893798828125\n",
      "Epoch 16, Generator Loss: -0.6813037991523743, Discriminator Loss: 8.997100830078125\n",
      "Epoch 16, Generator Loss: -0.690470814704895, Discriminator Loss: 9.007384300231934\n",
      "Epoch 16, Generator Loss: -0.6877319812774658, Discriminator Loss: 9.025533676147461\n",
      "Epoch 16, Generator Loss: -0.6851546168327332, Discriminator Loss: 9.002053260803223\n",
      "Epoch 16, Generator Loss: -0.688036322593689, Discriminator Loss: 9.00892162322998\n",
      "Epoch 16, Generator Loss: -0.6853315234184265, Discriminator Loss: 8.974365234375\n",
      "Epoch 16, Generator Loss: -0.6916531920433044, Discriminator Loss: 9.033754348754883\n",
      "Epoch 16, Generator Loss: -0.6935673952102661, Discriminator Loss: 8.953475952148438\n",
      "Epoch 16, Generator Loss: -0.6935935020446777, Discriminator Loss: 8.957769393920898\n",
      "Epoch 16, Generator Loss: -0.6939034461975098, Discriminator Loss: 8.997562408447266\n",
      "Epoch 16, Generator Loss: -0.6944255828857422, Discriminator Loss: 8.942895889282227\n",
      "Epoch 16, Generator Loss: -0.6934922337532043, Discriminator Loss: 8.988429069519043\n",
      "Epoch 16, Generator Loss: -0.6959247589111328, Discriminator Loss: 9.00323486328125\n",
      "Epoch 16, Generator Loss: -0.6949361562728882, Discriminator Loss: 8.971418380737305\n",
      "Epoch 16, Generator Loss: -0.6899486184120178, Discriminator Loss: 9.004194259643555\n",
      "Epoch 16, Generator Loss: -0.6952804923057556, Discriminator Loss: 8.94517707824707\n",
      "Epoch 16, Generator Loss: -0.6998885273933411, Discriminator Loss: 8.972779273986816\n",
      "Epoch 16, Generator Loss: -0.698034942150116, Discriminator Loss: 8.923669815063477\n",
      "Epoch 16, Generator Loss: -0.7059093713760376, Discriminator Loss: 8.98709487915039\n",
      "Epoch 16, Generator Loss: -0.7031573057174683, Discriminator Loss: 8.977021217346191\n",
      "Epoch 16, Generator Loss: -0.6963064074516296, Discriminator Loss: 8.94292163848877\n",
      "Epoch 16, Generator Loss: -0.699921727180481, Discriminator Loss: 8.907658576965332\n",
      "Epoch 16, Generator Loss: -0.7023125886917114, Discriminator Loss: 8.97815227508545\n",
      "Epoch 16, Generator Loss: -0.7027157545089722, Discriminator Loss: 8.941390037536621\n",
      "Epoch 16, Generator Loss: -0.6981244683265686, Discriminator Loss: 8.935495376586914\n",
      "Epoch 16, Generator Loss: -0.6979047060012817, Discriminator Loss: 8.921876907348633\n",
      "Epoch 16, Generator Loss: -0.7039452195167542, Discriminator Loss: 8.906527519226074\n",
      "Epoch 16, Generator Loss: -0.7104487419128418, Discriminator Loss: 8.964616775512695\n",
      "Epoch 16, Generator Loss: -0.7047621011734009, Discriminator Loss: 8.920764923095703\n",
      "Epoch 16, Generator Loss: -0.7087093591690063, Discriminator Loss: 8.964693069458008\n",
      "Epoch 16, Generator Loss: -0.705475926399231, Discriminator Loss: 8.93493938446045\n",
      "Epoch 16, Generator Loss: -0.7085302472114563, Discriminator Loss: 8.90359115600586\n",
      "Epoch 16, Generator Loss: -0.7089977264404297, Discriminator Loss: 8.9166259765625\n",
      "Epoch 16, Generator Loss: -0.7100080847740173, Discriminator Loss: 8.903480529785156\n",
      "Epoch 16, Generator Loss: -0.7143459320068359, Discriminator Loss: 8.89468002319336\n",
      "Epoch 16, Generator Loss: -0.7115488052368164, Discriminator Loss: 8.915508270263672\n",
      "Epoch 16, Generator Loss: -0.7171027660369873, Discriminator Loss: 8.842419624328613\n",
      "Epoch 16, Generator Loss: -0.7110291719436646, Discriminator Loss: 8.924211502075195\n",
      "Epoch 17, Generator Loss: -0.716326117515564, Discriminator Loss: 8.915992736816406\n",
      "Epoch 17, Generator Loss: -0.7167485356330872, Discriminator Loss: 8.893068313598633\n",
      "Epoch 17, Generator Loss: -0.7179022431373596, Discriminator Loss: 8.876729011535645\n",
      "Epoch 17, Generator Loss: -0.7105587720870972, Discriminator Loss: 8.876850128173828\n",
      "Epoch 17, Generator Loss: -0.7184100151062012, Discriminator Loss: 8.853638648986816\n",
      "Epoch 17, Generator Loss: -0.7151039838790894, Discriminator Loss: 8.893901824951172\n",
      "Epoch 17, Generator Loss: -0.7215721011161804, Discriminator Loss: 8.907832145690918\n",
      "Epoch 17, Generator Loss: -0.7197259068489075, Discriminator Loss: 8.86964225769043\n",
      "Epoch 17, Generator Loss: -0.7159374356269836, Discriminator Loss: 8.857172012329102\n",
      "Epoch 17, Generator Loss: -0.7209731340408325, Discriminator Loss: 8.825676918029785\n",
      "Epoch 17, Generator Loss: -0.7261674404144287, Discriminator Loss: 8.873559951782227\n",
      "Epoch 17, Generator Loss: -0.7217252254486084, Discriminator Loss: 8.81634521484375\n",
      "Epoch 17, Generator Loss: -0.7246387004852295, Discriminator Loss: 8.895012855529785\n",
      "Epoch 17, Generator Loss: -0.7236699461936951, Discriminator Loss: 8.865445137023926\n",
      "Epoch 17, Generator Loss: -0.7208219766616821, Discriminator Loss: 8.809433937072754\n",
      "Epoch 17, Generator Loss: -0.7223254442214966, Discriminator Loss: 8.829547882080078\n",
      "Epoch 17, Generator Loss: -0.7181987762451172, Discriminator Loss: 8.815999031066895\n",
      "Epoch 17, Generator Loss: -0.7256808876991272, Discriminator Loss: 8.889612197875977\n",
      "Epoch 17, Generator Loss: -0.7290574312210083, Discriminator Loss: 8.812215805053711\n",
      "Epoch 17, Generator Loss: -0.7313410639762878, Discriminator Loss: 8.827037811279297\n",
      "Epoch 17, Generator Loss: -0.7278437614440918, Discriminator Loss: 8.828694343566895\n",
      "Epoch 17, Generator Loss: -0.7334029674530029, Discriminator Loss: 8.8064603805542\n",
      "Epoch 17, Generator Loss: -0.72520911693573, Discriminator Loss: 8.848808288574219\n",
      "Epoch 17, Generator Loss: -0.7309712767601013, Discriminator Loss: 8.874190330505371\n",
      "Epoch 17, Generator Loss: -0.7303788661956787, Discriminator Loss: 8.828939437866211\n",
      "Epoch 17, Generator Loss: -0.733374834060669, Discriminator Loss: 8.86010456085205\n",
      "Epoch 17, Generator Loss: -0.7319060564041138, Discriminator Loss: 8.814783096313477\n",
      "Epoch 17, Generator Loss: -0.7288825511932373, Discriminator Loss: 8.822246551513672\n",
      "Epoch 17, Generator Loss: -0.7280076742172241, Discriminator Loss: 8.8149995803833\n",
      "Epoch 17, Generator Loss: -0.733333945274353, Discriminator Loss: 8.772644996643066\n",
      "Epoch 17, Generator Loss: -0.7319502830505371, Discriminator Loss: 8.814335823059082\n",
      "Epoch 17, Generator Loss: -0.7376388907432556, Discriminator Loss: 8.836424827575684\n",
      "Epoch 17, Generator Loss: -0.7321770191192627, Discriminator Loss: 8.78931999206543\n",
      "Epoch 17, Generator Loss: -0.7399616241455078, Discriminator Loss: 8.770831108093262\n",
      "Epoch 17, Generator Loss: -0.7418230772018433, Discriminator Loss: 8.815498352050781\n",
      "Epoch 17, Generator Loss: -0.7401268482208252, Discriminator Loss: 8.789257049560547\n",
      "Epoch 17, Generator Loss: -0.7426823973655701, Discriminator Loss: 8.768265724182129\n",
      "Epoch 17, Generator Loss: -0.7411472201347351, Discriminator Loss: 8.81832218170166\n",
      "Epoch 17, Generator Loss: -0.746610164642334, Discriminator Loss: 8.776961326599121\n",
      "Epoch 17, Generator Loss: -0.7412619590759277, Discriminator Loss: 8.780567169189453\n",
      "Epoch 17, Generator Loss: -0.7510986328125, Discriminator Loss: 8.774515151977539\n",
      "Epoch 17, Generator Loss: -0.7476613521575928, Discriminator Loss: 8.729680061340332\n",
      "Epoch 17, Generator Loss: -0.7485367059707642, Discriminator Loss: 8.743041038513184\n",
      "Epoch 17, Generator Loss: -0.752355694770813, Discriminator Loss: 8.736804008483887\n",
      "Epoch 17, Generator Loss: -0.7478507161140442, Discriminator Loss: 8.733802795410156\n",
      "Epoch 17, Generator Loss: -0.7463901042938232, Discriminator Loss: 8.724469184875488\n",
      "Epoch 17, Generator Loss: -0.7520937919616699, Discriminator Loss: 8.723691940307617\n",
      "Epoch 17, Generator Loss: -0.7467182874679565, Discriminator Loss: 8.7676420211792\n",
      "Epoch 17, Generator Loss: -0.7543587684631348, Discriminator Loss: 8.722302436828613\n",
      "Epoch 17, Generator Loss: -0.7460314035415649, Discriminator Loss: 8.77114486694336\n",
      "Epoch 18, Generator Loss: -0.7478302717208862, Discriminator Loss: 8.726129531860352\n",
      "Epoch 18, Generator Loss: -0.7489683628082275, Discriminator Loss: 8.712493896484375\n",
      "Epoch 18, Generator Loss: -0.7513688206672668, Discriminator Loss: 8.736827850341797\n",
      "Epoch 18, Generator Loss: -0.7459007501602173, Discriminator Loss: 8.678788185119629\n",
      "Epoch 18, Generator Loss: -0.7489337921142578, Discriminator Loss: 8.720261573791504\n",
      "Epoch 18, Generator Loss: -0.7492472529411316, Discriminator Loss: 8.73763656616211\n",
      "Epoch 18, Generator Loss: -0.7518367767333984, Discriminator Loss: 8.722728729248047\n",
      "Epoch 18, Generator Loss: -0.7529969215393066, Discriminator Loss: 8.798335075378418\n",
      "Epoch 18, Generator Loss: -0.7650238871574402, Discriminator Loss: 8.729803085327148\n",
      "Epoch 18, Generator Loss: -0.7567968964576721, Discriminator Loss: 8.694369316101074\n",
      "Epoch 18, Generator Loss: -0.7555490732192993, Discriminator Loss: 8.708721160888672\n",
      "Epoch 18, Generator Loss: -0.768649160861969, Discriminator Loss: 8.697396278381348\n",
      "Epoch 18, Generator Loss: -0.7597607374191284, Discriminator Loss: 8.741683006286621\n",
      "Epoch 18, Generator Loss: -0.768555760383606, Discriminator Loss: 8.753721237182617\n",
      "Epoch 18, Generator Loss: -0.7667205929756165, Discriminator Loss: 8.735662460327148\n",
      "Epoch 18, Generator Loss: -0.7631991505622864, Discriminator Loss: 8.70287799835205\n",
      "Epoch 18, Generator Loss: -0.762136697769165, Discriminator Loss: 8.732080459594727\n",
      "Epoch 18, Generator Loss: -0.7649917602539062, Discriminator Loss: 8.693092346191406\n",
      "Epoch 18, Generator Loss: -0.7635322213172913, Discriminator Loss: 8.665793418884277\n",
      "Epoch 18, Generator Loss: -0.7592644691467285, Discriminator Loss: 8.673582077026367\n",
      "Epoch 18, Generator Loss: -0.767606258392334, Discriminator Loss: 8.723179817199707\n",
      "Epoch 18, Generator Loss: -0.7682511806488037, Discriminator Loss: 8.737016677856445\n",
      "Epoch 18, Generator Loss: -0.7631405591964722, Discriminator Loss: 8.673523902893066\n",
      "Epoch 18, Generator Loss: -0.7719956636428833, Discriminator Loss: 8.716487884521484\n",
      "Epoch 18, Generator Loss: -0.7683808207511902, Discriminator Loss: 8.687609672546387\n",
      "Epoch 18, Generator Loss: -0.7696950435638428, Discriminator Loss: 8.683773040771484\n",
      "Epoch 18, Generator Loss: -0.769393801689148, Discriminator Loss: 8.647212982177734\n",
      "Epoch 18, Generator Loss: -0.7734426856040955, Discriminator Loss: 8.665529251098633\n",
      "Epoch 18, Generator Loss: -0.7691670060157776, Discriminator Loss: 8.649930000305176\n",
      "Epoch 18, Generator Loss: -0.7742464542388916, Discriminator Loss: 8.678642272949219\n",
      "Epoch 18, Generator Loss: -0.7674846649169922, Discriminator Loss: 8.631567001342773\n",
      "Epoch 18, Generator Loss: -0.7684875726699829, Discriminator Loss: 8.647771835327148\n",
      "Epoch 18, Generator Loss: -0.7731518745422363, Discriminator Loss: 8.679014205932617\n",
      "Epoch 18, Generator Loss: -0.7813735604286194, Discriminator Loss: 8.614151954650879\n",
      "Epoch 18, Generator Loss: -0.7768819332122803, Discriminator Loss: 8.67061996459961\n",
      "Epoch 18, Generator Loss: -0.7692956328392029, Discriminator Loss: 8.670217514038086\n",
      "Epoch 18, Generator Loss: -0.7820720672607422, Discriminator Loss: 8.613452911376953\n",
      "Epoch 18, Generator Loss: -0.7779684066772461, Discriminator Loss: 8.598288536071777\n",
      "Epoch 18, Generator Loss: -0.7793661952018738, Discriminator Loss: 8.62360668182373\n",
      "Epoch 18, Generator Loss: -0.7735695838928223, Discriminator Loss: 8.65088939666748\n",
      "Epoch 18, Generator Loss: -0.7756741046905518, Discriminator Loss: 8.578529357910156\n",
      "Epoch 18, Generator Loss: -0.7828651666641235, Discriminator Loss: 8.613985061645508\n",
      "Epoch 18, Generator Loss: -0.7800071239471436, Discriminator Loss: 8.649730682373047\n",
      "Epoch 18, Generator Loss: -0.7826461791992188, Discriminator Loss: 8.543251991271973\n",
      "Epoch 18, Generator Loss: -0.7880464792251587, Discriminator Loss: 8.610010147094727\n",
      "Epoch 18, Generator Loss: -0.7879915833473206, Discriminator Loss: 8.616781234741211\n",
      "Epoch 18, Generator Loss: -0.7843000888824463, Discriminator Loss: 8.604040145874023\n",
      "Epoch 18, Generator Loss: -0.7829062342643738, Discriminator Loss: 8.57987117767334\n",
      "Epoch 18, Generator Loss: -0.7883679866790771, Discriminator Loss: 8.614558219909668\n",
      "Epoch 18, Generator Loss: -0.7840964198112488, Discriminator Loss: 8.54612922668457\n",
      "Epoch 19, Generator Loss: -0.7868635654449463, Discriminator Loss: 8.619232177734375\n",
      "Epoch 19, Generator Loss: -0.790128231048584, Discriminator Loss: 8.619501113891602\n",
      "Epoch 19, Generator Loss: -0.7865657806396484, Discriminator Loss: 8.549468040466309\n",
      "Epoch 19, Generator Loss: -0.7901113033294678, Discriminator Loss: 8.601224899291992\n",
      "Epoch 19, Generator Loss: -0.7872428894042969, Discriminator Loss: 8.538189888000488\n",
      "Epoch 19, Generator Loss: -0.7968863844871521, Discriminator Loss: 8.651927947998047\n",
      "Epoch 19, Generator Loss: -0.7937607765197754, Discriminator Loss: 8.582022666931152\n",
      "Epoch 19, Generator Loss: -0.7932865023612976, Discriminator Loss: 8.613462448120117\n",
      "Epoch 19, Generator Loss: -0.7977465987205505, Discriminator Loss: 8.579733848571777\n",
      "Epoch 19, Generator Loss: -0.7904849648475647, Discriminator Loss: 8.65200138092041\n",
      "Epoch 19, Generator Loss: -0.8059753775596619, Discriminator Loss: 8.59271240234375\n",
      "Epoch 19, Generator Loss: -0.7977426052093506, Discriminator Loss: 8.524581909179688\n",
      "Epoch 19, Generator Loss: -0.7933157682418823, Discriminator Loss: 8.594569206237793\n",
      "Epoch 19, Generator Loss: -0.7841976284980774, Discriminator Loss: 8.504679679870605\n",
      "Epoch 19, Generator Loss: -0.800580620765686, Discriminator Loss: 8.509696006774902\n",
      "Epoch 19, Generator Loss: -0.7959656715393066, Discriminator Loss: 8.566363334655762\n",
      "Epoch 19, Generator Loss: -0.7986139059066772, Discriminator Loss: 8.551376342773438\n",
      "Epoch 19, Generator Loss: -0.800990879535675, Discriminator Loss: 8.492517471313477\n",
      "Epoch 19, Generator Loss: -0.799508810043335, Discriminator Loss: 8.55813980102539\n",
      "Epoch 19, Generator Loss: -0.8015264272689819, Discriminator Loss: 8.497274398803711\n",
      "Epoch 19, Generator Loss: -0.8008025884628296, Discriminator Loss: 8.553254127502441\n",
      "Epoch 19, Generator Loss: -0.8057187795639038, Discriminator Loss: 8.453958511352539\n",
      "Epoch 19, Generator Loss: -0.8065211176872253, Discriminator Loss: 8.542227745056152\n",
      "Epoch 19, Generator Loss: -0.800776481628418, Discriminator Loss: 8.54217529296875\n",
      "Epoch 19, Generator Loss: -0.8044730424880981, Discriminator Loss: 8.51250171661377\n",
      "Epoch 19, Generator Loss: -0.8112683296203613, Discriminator Loss: 8.512157440185547\n",
      "Epoch 19, Generator Loss: -0.8144527673721313, Discriminator Loss: 8.434480667114258\n",
      "Epoch 19, Generator Loss: -0.8076354265213013, Discriminator Loss: 8.480165481567383\n",
      "Epoch 19, Generator Loss: -0.8107266426086426, Discriminator Loss: 8.517590522766113\n",
      "Epoch 19, Generator Loss: -0.8140932321548462, Discriminator Loss: 8.49116325378418\n",
      "Epoch 19, Generator Loss: -0.821613073348999, Discriminator Loss: 8.568314552307129\n",
      "Epoch 19, Generator Loss: -0.8137258291244507, Discriminator Loss: 8.472309112548828\n",
      "Epoch 19, Generator Loss: -0.8136230707168579, Discriminator Loss: 8.48478889465332\n",
      "Epoch 19, Generator Loss: -0.8125301599502563, Discriminator Loss: 8.460524559020996\n",
      "Epoch 19, Generator Loss: -0.8150979280471802, Discriminator Loss: 8.433212280273438\n",
      "Epoch 19, Generator Loss: -0.8113524913787842, Discriminator Loss: 8.545512199401855\n",
      "Epoch 19, Generator Loss: -0.8131269812583923, Discriminator Loss: 8.404802322387695\n",
      "Epoch 19, Generator Loss: -0.8131915330886841, Discriminator Loss: 8.41644287109375\n",
      "Epoch 19, Generator Loss: -0.8177177309989929, Discriminator Loss: 8.497879028320312\n",
      "Epoch 19, Generator Loss: -0.8158011436462402, Discriminator Loss: 8.45604419708252\n",
      "Epoch 19, Generator Loss: -0.8172426223754883, Discriminator Loss: 8.520662307739258\n",
      "Epoch 19, Generator Loss: -0.8236216306686401, Discriminator Loss: 8.546801567077637\n",
      "Epoch 19, Generator Loss: -0.8222901821136475, Discriminator Loss: 8.496356964111328\n",
      "Epoch 19, Generator Loss: -0.8185268640518188, Discriminator Loss: 8.527643203735352\n",
      "Epoch 19, Generator Loss: -0.8219547271728516, Discriminator Loss: 8.513866424560547\n",
      "Epoch 19, Generator Loss: -0.8251223564147949, Discriminator Loss: 8.451438903808594\n",
      "Epoch 19, Generator Loss: -0.8193000555038452, Discriminator Loss: 8.461074829101562\n",
      "Epoch 19, Generator Loss: -0.8242167830467224, Discriminator Loss: 8.409113883972168\n",
      "Epoch 19, Generator Loss: -0.8238296508789062, Discriminator Loss: 8.492545127868652\n",
      "Epoch 19, Generator Loss: -0.8222721815109253, Discriminator Loss: 8.467978477478027\n",
      "Epoch 20, Generator Loss: -0.8211970925331116, Discriminator Loss: 8.553476333618164\n",
      "Epoch 20, Generator Loss: -0.8262146711349487, Discriminator Loss: 8.41908073425293\n",
      "Epoch 20, Generator Loss: -0.8229918479919434, Discriminator Loss: 8.423166275024414\n",
      "Epoch 20, Generator Loss: -0.826577365398407, Discriminator Loss: 8.522290229797363\n",
      "Epoch 20, Generator Loss: -0.8383516073226929, Discriminator Loss: 8.427592277526855\n",
      "Epoch 20, Generator Loss: -0.8255071640014648, Discriminator Loss: 8.552825927734375\n",
      "Epoch 20, Generator Loss: -0.828704833984375, Discriminator Loss: 8.428791999816895\n",
      "Epoch 20, Generator Loss: -0.8266271352767944, Discriminator Loss: 8.38768482208252\n",
      "Epoch 20, Generator Loss: -0.8384796380996704, Discriminator Loss: 8.375822067260742\n",
      "Epoch 20, Generator Loss: -0.8266944289207458, Discriminator Loss: 8.460821151733398\n",
      "Epoch 20, Generator Loss: -0.8325114250183105, Discriminator Loss: 8.396842956542969\n",
      "Epoch 20, Generator Loss: -0.8352755308151245, Discriminator Loss: 8.368876457214355\n",
      "Epoch 20, Generator Loss: -0.8335549831390381, Discriminator Loss: 8.321235656738281\n",
      "Epoch 20, Generator Loss: -0.8392549753189087, Discriminator Loss: 8.44638442993164\n",
      "Epoch 20, Generator Loss: -0.8343064785003662, Discriminator Loss: 8.495797157287598\n",
      "Epoch 20, Generator Loss: -0.8311493396759033, Discriminator Loss: 8.482551574707031\n",
      "Epoch 20, Generator Loss: -0.8333238959312439, Discriminator Loss: 8.501068115234375\n",
      "Epoch 20, Generator Loss: -0.8327852487564087, Discriminator Loss: 8.424604415893555\n",
      "Epoch 20, Generator Loss: -0.8436269164085388, Discriminator Loss: 8.313054084777832\n",
      "Epoch 20, Generator Loss: -0.8435318470001221, Discriminator Loss: 8.447816848754883\n",
      "Epoch 20, Generator Loss: -0.8324152231216431, Discriminator Loss: 8.411623001098633\n",
      "Epoch 20, Generator Loss: -0.8395431637763977, Discriminator Loss: 8.347037315368652\n",
      "Epoch 20, Generator Loss: -0.8317627906799316, Discriminator Loss: 8.402382850646973\n",
      "Epoch 20, Generator Loss: -0.8433687686920166, Discriminator Loss: 8.426721572875977\n",
      "Epoch 20, Generator Loss: -0.844640851020813, Discriminator Loss: 8.371194839477539\n",
      "Epoch 20, Generator Loss: -0.8368793725967407, Discriminator Loss: 8.416812896728516\n",
      "Epoch 20, Generator Loss: -0.8542112708091736, Discriminator Loss: 8.382610321044922\n",
      "Epoch 20, Generator Loss: -0.8341845273971558, Discriminator Loss: 8.449106216430664\n",
      "Epoch 20, Generator Loss: -0.8453346490859985, Discriminator Loss: 8.356359481811523\n",
      "Epoch 20, Generator Loss: -0.8397721648216248, Discriminator Loss: 8.323472023010254\n",
      "Epoch 20, Generator Loss: -0.8476859927177429, Discriminator Loss: 8.420682907104492\n",
      "Epoch 20, Generator Loss: -0.8455342054367065, Discriminator Loss: 8.306185722351074\n",
      "Epoch 20, Generator Loss: -0.8507821559906006, Discriminator Loss: 8.41544246673584\n",
      "Epoch 20, Generator Loss: -0.847083330154419, Discriminator Loss: 8.499185562133789\n",
      "Epoch 20, Generator Loss: -0.8478556275367737, Discriminator Loss: 8.345266342163086\n",
      "Epoch 20, Generator Loss: -0.8574026823043823, Discriminator Loss: 8.442036628723145\n",
      "Epoch 20, Generator Loss: -0.8561631441116333, Discriminator Loss: 8.362232208251953\n",
      "Epoch 20, Generator Loss: -0.8504838943481445, Discriminator Loss: 8.457021713256836\n",
      "Epoch 20, Generator Loss: -0.8465144634246826, Discriminator Loss: 8.279939651489258\n",
      "Epoch 20, Generator Loss: -0.8549733757972717, Discriminator Loss: 8.396504402160645\n",
      "Epoch 20, Generator Loss: -0.854735255241394, Discriminator Loss: 8.459552764892578\n",
      "Epoch 20, Generator Loss: -0.8545119166374207, Discriminator Loss: 8.294853210449219\n",
      "Epoch 20, Generator Loss: -0.8504315614700317, Discriminator Loss: 8.427620887756348\n",
      "Epoch 20, Generator Loss: -0.853047788143158, Discriminator Loss: 8.36151123046875\n",
      "Epoch 20, Generator Loss: -0.8536309003829956, Discriminator Loss: 8.278827667236328\n",
      "Epoch 20, Generator Loss: -0.8510372638702393, Discriminator Loss: 8.254005432128906\n",
      "Epoch 20, Generator Loss: -0.8591595888137817, Discriminator Loss: 8.349093437194824\n",
      "Epoch 20, Generator Loss: -0.8614330887794495, Discriminator Loss: 8.36415958404541\n",
      "Epoch 20, Generator Loss: -0.8554999232292175, Discriminator Loss: 8.26750373840332\n",
      "Epoch 20, Generator Loss: -0.8591214418411255, Discriminator Loss: 8.303634643554688\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m synthetic_data \u001b[38;5;241m=\u001b[39m tablegan\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m evaluation_score \u001b[38;5;241m=\u001b[39m \u001b[43mtablegan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluation_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[59], line 145\u001b[0m, in \u001b[0;36mTableGAN.evaluate\u001b[1;34m(self, x, y, model)\u001b[0m\n\u001b[0;32m    142\u001b[0m synthetic_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;28mlen\u001b[39m(x))\n\u001b[0;32m    143\u001b[0m synthetic_x, synthetic_y \u001b[38;5;241m=\u001b[39m synthetic_data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], synthetic_data[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 145\u001b[0m \u001b[43meval_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynthetic_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthetic_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m eval_model\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\tahat\\anaconda3\\envs\\chatbot\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tahat\\anaconda3\\envs\\chatbot\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1209\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1199\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m   1201\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1202\u001b[0m     X,\n\u001b[0;32m   1203\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1207\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1208\u001b[0m )\n\u001b[1;32m-> 1209\u001b[0m \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n\u001b[0;32m   1212\u001b[0m multi_class \u001b[38;5;241m=\u001b[39m _check_multi_class(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class, solver, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_))\n",
      "File \u001b[1;32mc:\\Users\\tahat\\anaconda3\\envs\\chatbot\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:221\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    213\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m ]:\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "# Determine the number of features (excluding the label column if it's included)\n",
    "input_dim = train_data.shape[1] - 1  # This should be 14\n",
    "generator_dims = [128, 256, 512]\n",
    "discriminator_dims = [512, 256, 128]\n",
    "\n",
    "# Initialize the TableGAN model\n",
    "tablegan = TableGAN(input_dim, generator_dims, discriminator_dims)\n",
    "\n",
    "# Prepare your data\n",
    "X = train_data[:, :-1]  # Features (should be shape (1600, 14))\n",
    "y = train_data[:, -1]   # Labels (should be shape (1600,))\n",
    "\n",
    "# Train the model\n",
    "tablegan.fit(X, y, epochs=20, batch_size=32, warmup_epochs=1, verbose=1)\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = tablegan.generate(1000)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_score = tablegan.evaluate(X, y, model='lr')\n",
    "print(f\"Evaluation score: {evaluation_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "n_samples = len(test_data)  # Number of samples to generate\n",
    "synthetic_data = tablegan.generate(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the last column is the label\n",
    "real_features = test_data[:, :-1]\n",
    "synthetic_features = synthetic_data[:, :-1]\n",
    "\n",
    "# Example: Compare distributions for the first feature\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(real_features[:, 0], bins=50, alpha=0.5, label='Real Data')\n",
    "plt.hist(synthetic_data[:, 0], bins=50, alpha=0.5, label='Synthetic Data')\n",
    "plt.legend()\n",
    "plt.title('Comparison of Real and Synthetic Data Distributions')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
